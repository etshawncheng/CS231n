{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data(zipped csv) from kaggle with username and apikey\n",
    "import os\n",
    "import json\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "with open(\"./kaggle.json\", \"r\") as j:\n",
    "    for (k, v) in json.load(j).items():\n",
    "        os.environ[k] = v\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "# https://www.kaggle.com/competitions/cafa-5-protein-function-prediction\n",
    "# datasetname\n",
    "api.dataset_download_files('arnabchaki/data-science-salaries-2023', path=\"./data/\", unzip=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3755, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_year</th>\n",
       "      <th>experience_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>job_title</th>\n",
       "      <th>salary</th>\n",
       "      <th>salary_currency</th>\n",
       "      <th>salary_in_usd</th>\n",
       "      <th>employee_residence</th>\n",
       "      <th>remote_ratio</th>\n",
       "      <th>company_location</th>\n",
       "      <th>company_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Principal Data Scientist</td>\n",
       "      <td>80000</td>\n",
       "      <td>EUR</td>\n",
       "      <td>85847</td>\n",
       "      <td>ES</td>\n",
       "      <td>100</td>\n",
       "      <td>ES</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>MI</td>\n",
       "      <td>CT</td>\n",
       "      <td>ML Engineer</td>\n",
       "      <td>30000</td>\n",
       "      <td>USD</td>\n",
       "      <td>30000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023</td>\n",
       "      <td>MI</td>\n",
       "      <td>CT</td>\n",
       "      <td>ML Engineer</td>\n",
       "      <td>25500</td>\n",
       "      <td>USD</td>\n",
       "      <td>25500</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>175000</td>\n",
       "      <td>USD</td>\n",
       "      <td>175000</td>\n",
       "      <td>CA</td>\n",
       "      <td>100</td>\n",
       "      <td>CA</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>120000</td>\n",
       "      <td>USD</td>\n",
       "      <td>120000</td>\n",
       "      <td>CA</td>\n",
       "      <td>100</td>\n",
       "      <td>CA</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   work_year experience_level employment_type                 job_title  \\\n",
       "0       2023               SE              FT  Principal Data Scientist   \n",
       "1       2023               MI              CT               ML Engineer   \n",
       "2       2023               MI              CT               ML Engineer   \n",
       "3       2023               SE              FT            Data Scientist   \n",
       "4       2023               SE              FT            Data Scientist   \n",
       "\n",
       "   salary salary_currency  salary_in_usd employee_residence  remote_ratio  \\\n",
       "0   80000             EUR          85847                 ES           100   \n",
       "1   30000             USD          30000                 US           100   \n",
       "2   25500             USD          25500                 US           100   \n",
       "3  175000             USD         175000                 CA           100   \n",
       "4  120000             USD         120000                 CA           100   \n",
       "\n",
       "  company_location company_size  \n",
       "0               ES            L  \n",
       "1               US            S  \n",
       "2               US            S  \n",
       "3               CA            M  \n",
       "4               CA            M  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_salaries = pd.read_csv(\"./data/ds_salaries.csv\")\n",
    "print(ds_salaries.shape)\n",
    "ds_salaries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_year</th>\n",
       "      <th>salary</th>\n",
       "      <th>salary_in_usd</th>\n",
       "      <th>remote_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3755.000000</td>\n",
       "      <td>3.755000e+03</td>\n",
       "      <td>3755.000000</td>\n",
       "      <td>3755.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022.373635</td>\n",
       "      <td>1.906956e+05</td>\n",
       "      <td>137570.389880</td>\n",
       "      <td>46.271638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.691448</td>\n",
       "      <td>6.716765e+05</td>\n",
       "      <td>63055.625278</td>\n",
       "      <td>48.589050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020.000000</td>\n",
       "      <td>6.000000e+03</td>\n",
       "      <td>5132.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2022.000000</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>95000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022.000000</td>\n",
       "      <td>1.380000e+05</td>\n",
       "      <td>135000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023.000000</td>\n",
       "      <td>1.800000e+05</td>\n",
       "      <td>175000.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2023.000000</td>\n",
       "      <td>3.040000e+07</td>\n",
       "      <td>450000.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         work_year        salary  salary_in_usd  remote_ratio\n",
       "count  3755.000000  3.755000e+03    3755.000000   3755.000000\n",
       "mean   2022.373635  1.906956e+05  137570.389880     46.271638\n",
       "std       0.691448  6.716765e+05   63055.625278     48.589050\n",
       "min    2020.000000  6.000000e+03    5132.000000      0.000000\n",
       "25%    2022.000000  1.000000e+05   95000.000000      0.000000\n",
       "50%    2022.000000  1.380000e+05  135000.000000      0.000000\n",
       "75%    2023.000000  1.800000e+05  175000.000000    100.000000\n",
       "max    2023.000000  3.040000e+07  450000.000000    100.000000"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_salaries.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['work_year: work_year\\n2023    1785\\n2022    1664\\n2021     230\\n2020      76\\ndtype: int64', 'experience_level: experience_level\\n3    2516\\n2     805\\n0     320\\n1     114\\ndtype: int64', 'employment_type: employment_type\\n2    3718\\n3      17\\n0      10\\n1      10\\ndtype: int64', 93, 815, 20, 1035, 78, 'remote_ratio: remote_ratio\\n0      1923\\n100    1643\\n50      189\\ndtype: int64', 72, 'company_size: company_size\\n1    3153\\n0     454\\n2     148\\ndtype: int64']\n"
     ]
    }
   ],
   "source": [
    "print([len(ds_salaries.value_counts(c))\n",
    "       if len(ds_salaries.value_counts(c)) > 10\n",
    "       else f\"{c}: {ds_salaries.value_counts(c)}\"\n",
    "       for c in ds_salaries.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['work_year', 'experience_level', 'employment_type', 'job_title',\n",
       "       'salary', 'salary_currency', 'salary_in_usd', 'employee_residence',\n",
       "       'remote_ratio', 'company_location', 'company_size'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_salaries.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "work_year              int64\n",
       "experience_level      object\n",
       "employment_type       object\n",
       "job_title             object\n",
       "salary                 int64\n",
       "salary_currency       object\n",
       "salary_in_usd          int64\n",
       "employee_residence    object\n",
       "remote_ratio           int64\n",
       "company_location      object\n",
       "company_size          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_salaries.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonnumerical_column_encoders = {\n",
    "    c: LabelEncoder() for c, dt in ds_salaries.dtypes.items() if dt == 'O'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_salaries[list(nonnumerical_column_encoders.keys())] = pd.DataFrame(\n",
    "    e.fit_transform(ds_salaries[c]) for c, e in nonnumerical_column_encoders.items()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "work_year             int64\n",
       "experience_level      int32\n",
       "employment_type       int32\n",
       "job_title             int32\n",
       "salary                int64\n",
       "salary_currency       int32\n",
       "salary_in_usd         int64\n",
       "employee_residence    int32\n",
       "remote_ratio          int64\n",
       "company_location      int32\n",
       "company_size          int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_salaries.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['work_year', 'experience_level', 'employment_type', 'job_title',\n",
       "       'salary', 'salary_currency', 'salary_in_usd', 'employee_residence',\n",
       "       'remote_ratio', 'company_location', 'company_size'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_salaries.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['work_year', 'experience_level', 'employment_type', 'job_title',\n",
       "       'employee_residence', 'remote_ratio', 'company_location',\n",
       "       'company_size'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_salaries.columns[:4].append(ds_salaries.columns[7:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3755"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_salaries.salary_in_usd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3755"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_salaries.salary_in_usd.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 85847,  30000,  25500,  ..., 105000, 100000,  94665])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(ds_salaries.salary_in_usd.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VS_Workplace\\NN\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim, Generator\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from numpy.random import choice\n",
    "from typing import Iterable, Callable, Type, Optional, Union, Tuple, List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import mul\n",
    "\n",
    "\n",
    "def product(nums: Iterable[Type], func: Callable[[Type, Type], Type] = mul) -> Type:\n",
    "    \"\"\"return product of iterable\"\"\"\n",
    "    _it = iter(nums)\n",
    "    v: Type = next(_it)\n",
    "    for _v in _it:\n",
    "        v = func(v, _v)\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DS_SalaryDataset(Dataset):\n",
    "    \"\"\"DS Salary dataset.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        ds_salaries: pd.DataFrame = pd.read_csv(\"./data/ds_salaries.csv\")\n",
    "        self.nonnumerical_column_encoders = {\n",
    "            c: LabelEncoder() for c, dt in ds_salaries.dtypes.items() if dt == 'O'}\n",
    "        ds_salaries[list(self.nonnumerical_column_encoders.keys())] = pd.DataFrame(\n",
    "            e.fit_transform(ds_salaries[c]) for c, e in self.nonnumerical_column_encoders.items()).T\n",
    "        #\n",
    "        self.feature = torch.from_numpy(\n",
    "            ds_salaries[ds_salaries.columns[:4].append(ds_salaries.columns[7:])].to_numpy(dtype=np.float32))\n",
    "        # two status\n",
    "        self.salary = torch.reshape(torch.tensor(\n",
    "            ds_salaries.salary_in_usd.to_numpy()), shape=(ds_salaries.salary_in_usd.size,))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.salary.size()[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self.feature[idx], self.salary[idx]\n",
    "\n",
    "\n",
    "def getCustomizedData():\n",
    "    # preprocess\n",
    "    dataset = DS_SalaryDataset()\n",
    "    # train test split\n",
    "    train_count = int(0.7 * len(dataset))\n",
    "    valid_count = int(0.2 * len(dataset))\n",
    "    test_count = len(dataset) - train_count - valid_count\n",
    "    print(train_count, valid_count, test_count)\n",
    "    trainset, valset, testset = random_split(\n",
    "        dataset, (train_count, valid_count, test_count), Generator().manual_seed(42))\n",
    "    datum_size = product(trainset[0][0].size())\n",
    "    return trainset, valset, testset, datum_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "\n",
    "class TwoLayerNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_classes: int, init_method: Callable[[torch.Tensor], torch.Tensor], active_func: Callable[[], nn.modules.module.Module],\n",
    "                 DO: float, if_BN: bool, store_size: int = 1):\n",
    "        super(TwoLayerNetwork, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.if_BN = if_BN\n",
    "        # dropout\n",
    "        self.do = nn.Dropout(DO)\n",
    "        # first layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # batch norm\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        # activation\n",
    "        self.active_func = active_func()\n",
    "        # second layer\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        # initialize\n",
    "        for param in self.parameters():\n",
    "            init_method(param)\n",
    "        self.storage: deque[List[nn.Parameter]] = deque(maxlen=store_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out: torch.Tensor = self.do(x)\n",
    "        out = self.fc1(out)\n",
    "        if self.if_BN:\n",
    "            out = self.bn1(out)\n",
    "        out = self.active_func(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WD_Regularization(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WD_Regularization, self).__init__()\n",
    "\n",
    "\n",
    "class L2_Regularization(WD_Regularization):\n",
    "    def __init__(self, weight_decay: float):\n",
    "        super(L2_Regularization, self).__init__()\n",
    "        if weight_decay <= 0:\n",
    "            raise ValueError(\"param weight_decay can not <=0!!\")\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, model: nn.Module) -> Union[torch.Tensor, float]:\n",
    "        reg = 0\n",
    "        for name, parameter in model.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                reg += torch.sum(parameter**2)\n",
    "        return self.weight_decay * reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model: TwoLayerNetwork, device: str, valset: Dataset[torch.Tensor], criterion: nn.modules.loss._Loss) \\\n",
    "        -> Tuple[float, float]:\n",
    "    \"\"\"return loss, accuracy\"\"\"\n",
    "    # Validate the model\n",
    "    model.to(device)\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in DataLoader(valset, batch_size=32, shuffle=True):\n",
    "            x: torch.Tensor = x.view(-1, model.input_size).to(device)\n",
    "            y: torch.Tensor = y.to(device)\n",
    "            outputs: torch.Tensor = model(x)\n",
    "            loss: torch.Tensor = criterion(outputs, y)\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_correct += (predicted == y).sum().item()\n",
    "        val_loss /= len(valset)\n",
    "        val_accuracy = val_correct / len(valset)\n",
    "    return val_loss, val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: TwoLayerNetwork, opt: Callable[..., optim.Optimizer], device: str, epochs: float, learning_rate: float, trainset: Dataset[torch.Tensor], valset: Dataset[torch.Tensor], criterion: nn.modules.loss._Loss,\n",
    "          sched: Optional[Callable[[optim.Optimizer], optim.lr_scheduler._LRScheduler]], wd_reg: Optional[WD_Regularization], learning_goal: float, min_lr: float, if_lr_adjust: bool, if_BN: bool, drop_rate: float) \\\n",
    "        -> List[Tuple[float, float, float, float]]:\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        model\n",
    "        opt\n",
    "        device\n",
    "        epochs\n",
    "        learing_rate\n",
    "        criterion\n",
    "        y: label of data\n",
    "        wd_reg, BN, DO: regularization\n",
    "    Results:\n",
    "        history: train_loss, train_accuracy, val_loss, val_accuracy of each epochs\n",
    "    \"\"\"\n",
    "    def forward_backward(optimizer: optim.Optimizer, criterion: nn.modules.loss._Loss, wd_reg: Optional[WD_Regularization], model: TwoLayerNetwork, y: torch.Tensor,\n",
    "                         BN: Optional[nn.modules.batchnorm._BatchNorm], DO: Optional[nn.modules.dropout._DropoutNd]) \\\n",
    "            -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            optimizer\n",
    "            criterion\n",
    "            model\n",
    "            y: label of data\n",
    "            wd_reg, BN, DO: regularization\n",
    "        Results:\n",
    "            ouputs: f(x)\n",
    "            loss_all: f(x) - y\n",
    "        \"\"\"\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        outputs = outputs if not DO else DO(outputs)\n",
    "        loss_all: torch.Tensor = criterion(\n",
    "            outputs, y) + wd_reg(model) if wd_reg else criterion(outputs, y)\n",
    "        loss_all.backward()\n",
    "        optimizer.step()\n",
    "        return loss_all, outputs\n",
    "    if epochs < 1:\n",
    "        raise ValueError(\"Invalid epoch!!\")\n",
    "    if not 0 <= drop_rate < 1:\n",
    "        raise ValueError(\"Invalid dropout rate!!\")\n",
    "    # init\n",
    "    epoch = 0\n",
    "    init_lr = learning_rate\n",
    "    origin_if_BN = model.if_BN\n",
    "    model.if_BN = if_BN\n",
    "    pre_loss = float(\"inf\") if if_lr_adjust else None\n",
    "    batch_norm = nn.BatchNorm1d(model.hidden_size).to(\n",
    "        device) if if_BN else None\n",
    "    drop_out = nn.Dropout(drop_rate).to(device) if drop_rate != 0. else None\n",
    "    model.to(device)\n",
    "    # if not model.storage[-1]\n",
    "    model.storage.append(list(model.parameters()))\n",
    "    optimizer = opt(model.storage[-1], lr=learning_rate)\n",
    "    scheduler = sched(optimizer) if sched else None\n",
    "    history = []\n",
    "    # Train the model\n",
    "    while epoch < epochs:\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        model.train()\n",
    "        for x, y in DataLoader(trainset, batch_size=32, shuffle=True):\n",
    "            x: torch.Tensor = x.view(-1, model.input_size).to(device)\n",
    "            y: torch.Tensor = y.to(device)\n",
    "            loss_all, outputs = forward_backward(\n",
    "                optimizer, criterion, wd_reg, model, y, batch_norm, drop_out)\n",
    "            # Learning rate adjustment\n",
    "            if pre_loss:\n",
    "                while pre_loss <= loss_all.item():\n",
    "                    # learning rate vanishing\n",
    "                    if learning_rate < min_lr:\n",
    "                        # return history\n",
    "                        learning_rate = init_lr\n",
    "                        optimizer = opt(model.storage[-1], lr=learning_rate)\n",
    "                        loss_all, outputs = forward_backward(\n",
    "                            optimizer, criterion, wd_reg, model, y, batch_norm, drop_out)\n",
    "                        # raise ValueError(f\"{learning_rate} < {min_lr}\")\n",
    "                        break\n",
    "                    learning_rate *= 0.7\n",
    "                    optimizer = opt(model.storage[-1], lr=learning_rate)\n",
    "                    loss_all, outputs = forward_backward(\n",
    "                        optimizer, criterion, wd_reg, model, y, batch_norm, drop_out)\n",
    "                learning_rate *= 1.2\n",
    "                pre_loss = loss_all.item()\n",
    "            train_loss += loss_all.item() * x.size(0)\n",
    "            predicted: torch.Tensor = torch.max(outputs.data, 1)[1]\n",
    "            train_correct += (predicted == y).sum().item()\n",
    "            model.storage.append(list(model.parameters()))\n",
    "        train_loss /= len(trainset)\n",
    "        train_accuracy = train_correct / len(trainset)\n",
    "        # Validate the model\n",
    "        val_loss, val_accuracy = validate(\n",
    "            model=model, device=device, valset=valset, criterion=criterion)\n",
    "        # Log statics\n",
    "        history.append((train_loss, train_accuracy, val_loss, val_accuracy))\n",
    "        # Stopping criteria\n",
    "        if learning_goal < val_accuracy:\n",
    "            return history\n",
    "        # Update loop\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        epoch += 1\n",
    "    # restore model\n",
    "    model.if_BN = origin_if_BN\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model: TwoLayerNetwork, device: str, testset: Dataset[torch.Tensor]) -> float:\n",
    "    \"\"\"return accuracy\"\"\"\n",
    "    return validate(model=model, device=device, valset=testset, criterion=nn.CrossEntropyLoss())[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogizing(model: TwoLayerNetwork, device: str, trainset: Dataset[torch.Tensor], learning_goal: float, criterion: nn.modules.loss._Loss):\n",
    "    x = torch.stack([x for x, _ in trainset]\n",
    "                    ).view(-1, model.input_size).to(\"cpu\")\n",
    "    y = torch.Tensor([y for _, y in trainset]).to(\"cpu\")\n",
    "    total_amount = len(x)\n",
    "    # get wrong correct indices\n",
    "    new_fc1_w = model.fc1.weight.data.to(device)\n",
    "    new_fc1_b = model.fc1.bias.data.to(device)\n",
    "    new_fc2_w = model.fc2.weight.data.to(device)\n",
    "    relu = nn.ReLU()\n",
    "    logits: torch.Tensor = x\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = relu(x.to(device) @ new_fc1_w.T + new_fc1_b\n",
    "                       ) @ new_fc2_w.T + model.fc2.bias.data.to(device)\n",
    "        logits, predicted = torch.max(outputs.data, 1)\n",
    "        success_condition = predicted == y.to(device)\n",
    "        wrong_indices = torch.nonzero(success_condition != True).to(\"cpu\")\n",
    "    #\n",
    "    init_func: Callable[[torch.Tensor], torch.Tensor] = lambda x: nn.init.xavier_uniform_(\n",
    "        tensor=x) if len(x.shape) > 1 else x\n",
    "    wrong_pointer = len(wrong_indices)\n",
    "    train_correct: int = 0\n",
    "    loss: float = float(\"inf\")\n",
    "    history: List[Tuple[float, float]] = []\n",
    "    while wrong_pointer > 0 and loss > learning_goal:\n",
    "        fc1 = nn.Linear(len(x[0]), 3).to(device)\n",
    "        fc2 = nn.Linear(3, product(model.fc2.bias.size()),\n",
    "                        False).to(device)\n",
    "        # with torch.no_grad():\n",
    "        target = torch.zeros(*torch.Size((total_amount,)))\n",
    "        wrong_pointer -= 1\n",
    "        pointer = wrong_indices[wrong_pointer]\n",
    "        catagory = int(y[pointer])\n",
    "        target[pointer] = catagory\n",
    "        target = target.to(device)\n",
    "        fc2.weight.data[:, :] = 0\n",
    "        fc2.weight.data[catagory, 0] = -2\n",
    "        fc2.weight.data[catagory, 1] = 1\n",
    "        fc2.weight.data[catagory, 2] = 1\n",
    "        delta = 0\n",
    "        intercept = 0\n",
    "        nonz = x\n",
    "        # randomly generate hyperplane which only contain the target x\n",
    "        while nonz.size() != (1, 2) or nonz.tolist()[0][0] != pointer:\n",
    "            for p in fc1.parameters():\n",
    "                init_func(p)\n",
    "            distances = x.to(device) @ fc1.weight.data[0].T\n",
    "            intercept = distances[pointer]\n",
    "            distances -= intercept\n",
    "            # get the shortest distance of other x to hyperplane\n",
    "            if (delta := torch.min(torch.abs(torch.cat(\n",
    "                    (distances[:pointer], distances[pointer + 1:])\n",
    "            )))) == 0:\n",
    "                continue\n",
    "            fc1.bias.data[1] = -intercept + (delta / 2)\n",
    "            fc1.bias.data[2] = -intercept - (delta / 2)\n",
    "            # check if delta too small for float32(default)\n",
    "            if fc1.bias.data[1] == fc1.bias.data[2]:\n",
    "                continue\n",
    "            fc1.bias.data[0] = -intercept\n",
    "            fc1.weight.data[1:] = fc1.weight.data[0]\n",
    "            outputs = relu(x.to(device) @ fc1.weight.data.T + fc1.bias.data\n",
    "                           ) @ fc2.weight.data.T\n",
    "            nonz = torch.nonzero(outputs)\n",
    "        # adjust weight in order to make the output of correct category greater than the others\n",
    "        fc2.weight.data *= logits[pointer].item(\n",
    "        ) / outputs[pointer].sum() + 1\n",
    "        new_fc1_w = torch.cat((new_fc1_w, fc1.weight.data)).to(device)\n",
    "        new_fc1_b = torch.cat((new_fc1_b, fc1.bias.data)).to(device)\n",
    "        new_fc2_w = torch.cat((new_fc2_w, fc2.weight.data), 1).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = relu(x.to(device) @ new_fc1_w.T + new_fc1_b\n",
    "                           ) @ new_fc2_w.T + model.fc2.bias.data.to(device)\n",
    "            loss = criterion(\n",
    "                outputs, y.to(device=device, dtype=torch.long)).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_correct = (predicted == y.to(device)).sum().item()\n",
    "        history.append((loss, train_correct / total_amount))\n",
    "    # construct new model\n",
    "    new_model = TwoLayerNetwork(model.input_size, len(new_fc1_b), product(\n",
    "        model.fc2.bias.size()), lambda _: _, lambda: model.active_func, model.do.p, model.if_BN)\n",
    "    for name, param in model.named_parameters():\n",
    "        layer_name, variable_type = name.split(\".\")\n",
    "        if layer_name == \"fc1\":\n",
    "            setattr(getattr(getattr(new_model, layer_name), variable_type),\n",
    "                    \"data\", eval(f\"new_{layer_name}_{variable_type[0]}\"))\n",
    "        elif layer_name == \"fc2\":\n",
    "            if variable_type == \"weight\":\n",
    "                setattr(getattr(getattr(new_model, layer_name), variable_type),\n",
    "                        \"data\", eval(f\"new_{layer_name}_{variable_type[0]}\"))\n",
    "            elif variable_type == \"bias\":\n",
    "                new_model.fc2.bias.data[:] = model.fc2.bias.data[:]\n",
    "            else:\n",
    "                pass\n",
    "                setattr(getattr(new_model, layer_name), variable_type, param)\n",
    "        else:\n",
    "            setattr(getattr(new_model, layer_name), variable_type, param)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2628 751 376\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available(\n",
    ") else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "trainset, valset, testset, input_size = getCustomizedData()\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 11\n",
    "epochs = 300\n",
    "init: Callable[[torch.Tensor], torch.Tensor] = lambda x: nn.init.xavier_uniform_(\n",
    "    tensor=x) if len(x.shape) > 1 else x\n",
    "active = nn.ReLU\n",
    "model = TwoLayerNetwork(input_size, hidden_size, 1, init, active, 0., False)\n",
    "optimize = optim.SGD\n",
    "schedule = None\n",
    "learning_goal = 0.6 #\n",
    "learning_rate = 0.001\n",
    "min_lr = learning_rate * 1e-5\n",
    "l2_reg = L2_Regularization(0.001)\n",
    "RG_EB_LG_UA_BN_DO_baseline = test(model, device, testset)\n",
    "RG_EB_LG_UA_BN_DO_history = train(model, optimize, device, epochs, learning_rate,\n",
    "                   trainset, valset, criterion, schedule, l2_reg, learning_goal, min_lr, True, False, 0.)\n",
    "RG_EB_LG_UA_BN_DO_result = test(model, device, testset)\n",
    "print(RG_EB_LG_UA_BN_DO_baseline, RG_EB_LG_UA_BN_DO_history, RG_EB_LG_UA_BN_DO_result, sep=\"\\n\")\n",
    "model_path = r\"./data/rg_eb_lg_ua_\"\n",
    "torch.save(model, model_path)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

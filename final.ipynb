{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim, Generator\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from numpy.random import choice\n",
    "from typing import Iterable, Callable, Type, Optional, Union, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data(zipped csv) from kaggle with username and apikey\n",
    "# import os\n",
    "# import json\n",
    "# from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "# with open(\"./kaggle.json\", \"r\") as j:\n",
    "#     for (k, v) in json.load(j).items():\n",
    "#         os.environ[k] = v\n",
    "# api = KaggleApi()\n",
    "# api.authenticate()\n",
    "# # https://www.kaggle.com/competitions/cafa-5-protein-function-prediction\n",
    "# # datasetname\n",
    "# api.dataset_download_files('arnabchaki/data-science-salaries-2023', path=\"./data/\", unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import mul\n",
    "\n",
    "\n",
    "def product(nums: Iterable[Type], func: Callable[[Type, Type], Type] = mul) -> Type:\n",
    "    \"\"\"return product of iterable\"\"\"\n",
    "    _it = iter(nums)\n",
    "    v: Type = next(_it)\n",
    "    for _v in _it:\n",
    "        v = func(v, _v)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_salaries: pd.DataFrame = pd.read_csv(\"./data/ds_salaries.csv\")\n",
    "nonnumerical_column_encoders = {\n",
    "    c: LabelEncoder() for c, dt in ds_salaries.dtypes.items() if dt == 'O'}\n",
    "ds_salaries[list(nonnumerical_column_encoders.keys())] = pd.DataFrame(\n",
    "    e.fit_transform(ds_salaries[c]) for c, e in nonnumerical_column_encoders.items()).T\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = torch.from_numpy(\n",
    "    ds_salaries[ds_salaries.columns[:4].append(ds_salaries.columns[7:])].to_numpy(dtype=np.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0230e+03, 3.0000e+00, 2.0000e+00,  ..., 1.0000e+02, 2.5000e+01,\n",
       "         0.0000e+00],\n",
       "        [2.0230e+03, 2.0000e+00, 0.0000e+00,  ..., 1.0000e+02, 7.0000e+01,\n",
       "         2.0000e+00],\n",
       "        [2.0230e+03, 2.0000e+00, 0.0000e+00,  ..., 1.0000e+02, 7.0000e+01,\n",
       "         2.0000e+00],\n",
       "        ...,\n",
       "        [2.0200e+03, 0.0000e+00, 2.0000e+00,  ..., 1.0000e+02, 7.0000e+01,\n",
       "         2.0000e+00],\n",
       "        [2.0200e+03, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+02, 7.0000e+01,\n",
       "         0.0000e+00],\n",
       "        [2.0210e+03, 3.0000e+00, 2.0000e+00,  ..., 5.0000e+01, 3.8000e+01,\n",
       "         0.0000e+00]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9776e-01, 1.4796e-03, 9.8642e-04,  ..., 4.9321e-02, 1.2330e-02,\n",
       "         0.0000e+00],\n",
       "        [9.9697e-01, 9.8564e-04, 0.0000e+00,  ..., 4.9282e-02, 3.4497e-02,\n",
       "         9.8564e-04],\n",
       "        [9.9697e-01, 9.8564e-04, 0.0000e+00,  ..., 4.9282e-02, 3.4497e-02,\n",
       "         9.8564e-04],\n",
       "        ...,\n",
       "        [9.9722e-01, 0.0000e+00, 9.8735e-04,  ..., 4.9368e-02, 3.4557e-02,\n",
       "         9.8735e-04],\n",
       "        [9.9746e-01, 0.0000e+00, 0.0000e+00,  ..., 4.9379e-02, 3.4565e-02,\n",
       "         0.0000e+00],\n",
       "        [9.9908e-01, 1.4831e-03, 9.8870e-04,  ..., 2.4718e-02, 1.8785e-02,\n",
       "         0.0000e+00]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.normalize(feature, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary = torch.reshape(torch.tensor(\n",
    "    ds_salaries.salary_in_usd.to_numpy(dtype=np.float32)), shape=(ds_salaries.salary_in_usd.size,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0093, 0.0032, 0.0027,  ..., 0.0113, 0.0108, 0.0102])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.normalize(salary, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class DS_SalaryDataset(Dataset):\n",
    "    \"\"\"DS Salary dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_path=\"./data/ds_salaries.csv\", transform=transforms.transforms.Normalize(0.5, 0.5)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        ds_salaries: pd.DataFrame = pd.read_csv(csv_path)\n",
    "        self.nonnumerical_column_encoders = {\n",
    "            c: LabelEncoder() for c, dt in ds_salaries.dtypes.items() if dt == 'O'}\n",
    "        ds_salaries[list(self.nonnumerical_column_encoders.keys())] = pd.DataFrame(\n",
    "            e.fit_transform(ds_salaries[c]) for c, e in self.nonnumerical_column_encoders.items()).T\n",
    "        #\n",
    "        self.feature = nn.functional.normalize(torch.from_numpy(\n",
    "            ds_salaries[ds_salaries.columns[:4].append(ds_salaries.columns[7:])].to_numpy(dtype=np.float32)), dim=1)\n",
    "\n",
    "        # target\n",
    "        self.salary = nn.functional.normalize(torch.reshape(torch.tensor(\n",
    "            ds_salaries.salary_in_usd.to_numpy(dtype=np.float32)), shape=(ds_salaries.salary_in_usd.size,)), dim=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.salary.size()[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self.feature[idx], self.salary[idx]\n",
    "\n",
    "\n",
    "def getCustomizedData():\n",
    "    # preprocess\n",
    "    dataset = DS_SalaryDataset()\n",
    "    # train test split\n",
    "    train_count = int(0.7 * len(dataset))\n",
    "    valid_count = int(0.2 * len(dataset))\n",
    "    test_count = len(dataset) - train_count - valid_count\n",
    "    print(train_count, valid_count, test_count)\n",
    "    trainset, valset, testset = random_split(\n",
    "        dataset, (train_count, valid_count, test_count), Generator().manual_seed(42))\n",
    "    datum_size = product(trainset[0][0].size())\n",
    "    return trainset, valset, testset, datum_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2628 751 376\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataset.Subset at 0x14f9d7797f0>,\n",
       " <torch.utils.data.dataset.Subset at 0x14f9d779a60>,\n",
       " <torch.utils.data.dataset.Subset at 0x14f9d779f40>,\n",
       " 8)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getCustomizedData()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "\n",
    "class TwoLayerNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_classes: int, init_method: Callable[[torch.Tensor], torch.Tensor], active_func: Callable[[], nn.modules.module.Module],\n",
    "                 DO: float, if_BN: bool, store_size: int = 1):\n",
    "        super(TwoLayerNetwork, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.if_BN = if_BN\n",
    "        # dropout\n",
    "        self.do = nn.Dropout(DO)\n",
    "        # first layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # batch norm\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        # activation\n",
    "        self.active_func = active_func()\n",
    "        # second layer\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        # initialize\n",
    "        for param in self.parameters():\n",
    "            init_method(param)\n",
    "        self.storage: deque[List[nn.Parameter]] = deque(maxlen=store_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out: torch.Tensor = self.do(x)\n",
    "        out = self.fc1(out)\n",
    "        if self.if_BN:\n",
    "            out = self.bn1(out)\n",
    "        out = self.active_func(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WD_Regularization(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WD_Regularization, self).__init__()\n",
    "\n",
    "\n",
    "class L2_Regularization(WD_Regularization):\n",
    "    def __init__(self, weight_decay: float):\n",
    "        super(L2_Regularization, self).__init__()\n",
    "        if weight_decay <= 0:\n",
    "            raise ValueError(\"param weight_decay can not <=0!!\")\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, model: nn.Module) -> Union[torch.Tensor, float]:\n",
    "        reg = 0\n",
    "        for name, parameter in model.named_parameters():\n",
    "            if name in (\"fc1.weight\", \"fc2.weight\"):\n",
    "                reg += torch.sum(parameter**2)\n",
    "        return self.weight_decay * reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model: TwoLayerNetwork, device: str, valset: Dataset[torch.Tensor], criterion: nn.modules.loss._Loss) \\\n",
    "        -> Tuple[float, float]:\n",
    "    \"\"\"return loss, accuracy\"\"\"\n",
    "    # Validate the model\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in DataLoader(valset, batch_size=32, shuffle=True):\n",
    "            x: torch.Tensor = x.view(-1, model.input_size).to(device)\n",
    "            y: torch.Tensor = y.to(device)\n",
    "            outputs: torch.Tensor = model(x)\n",
    "            loss: torch.Tensor = criterion(outputs, y)\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "        val_loss /= len(valset)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: TwoLayerNetwork, opt: Callable[..., optim.Optimizer], device: str, epochs: float, learning_rate: float, trainset: Dataset[torch.Tensor], valset: Dataset[torch.Tensor], criterion: nn.modules.loss._Loss,\n",
    "          sched: Optional[Callable[[optim.Optimizer], optim.lr_scheduler._LRScheduler]], wd_reg: Optional[WD_Regularization], learning_goal: float, min_lr: float, if_lr_adjust: bool, if_BN: bool, drop_rate: float) \\\n",
    "        -> List[Tuple[float, float, float, float]]:\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        model\n",
    "        opt\n",
    "        device\n",
    "        epochs\n",
    "        learing_rate\n",
    "        criterion\n",
    "        y: label of data\n",
    "        wd_reg, BN, DO: regularization\n",
    "    Results:\n",
    "        history: train_loss, train_accuracy, val_loss, val_accuracy of each epochs\n",
    "    \"\"\"\n",
    "    def forward_backward(optimizer: optim.Optimizer, criterion: nn.modules.loss._Loss, wd_reg: Optional[WD_Regularization], model: TwoLayerNetwork, y: torch.Tensor,\n",
    "                         BN: Optional[nn.modules.batchnorm._BatchNorm], DO: Optional[nn.modules.dropout._DropoutNd]) \\\n",
    "            -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            optimizer\n",
    "            criterion\n",
    "            model\n",
    "            y: label of data\n",
    "            wd_reg, BN, DO: regularization\n",
    "        Results:\n",
    "            ouputs: f(x)\n",
    "            loss_all: f(x) - y\n",
    "        \"\"\"\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        outputs = outputs if not DO else DO(outputs)\n",
    "        loss_all: torch.Tensor = criterion(\n",
    "            outputs, y) + wd_reg(model) if wd_reg else criterion(outputs, y)\n",
    "        loss_all.backward()\n",
    "        optimizer.step()\n",
    "        return loss_all, outputs\n",
    "    if epochs < 1:\n",
    "        raise ValueError(\"Invalid epoch!!\")\n",
    "    if not 0 <= drop_rate < 1:\n",
    "        raise ValueError(\"Invalid dropout rate!!\")\n",
    "    # init\n",
    "    epoch = 0\n",
    "    init_lr = learning_rate\n",
    "    origin_if_BN = model.if_BN\n",
    "    model.if_BN = if_BN\n",
    "    pre_loss = float(\"inf\") if if_lr_adjust else None\n",
    "    batch_norm = nn.BatchNorm1d(model.hidden_size).to(\n",
    "        device) if if_BN else None\n",
    "    drop_out = nn.Dropout(drop_rate).to(device) if drop_rate != 0. else None\n",
    "    model.to(device)\n",
    "    # if not model.storage[-1]\n",
    "    model.storage.append(list(model.parameters()))\n",
    "    optimizer = opt(model.storage[-1], lr=learning_rate)\n",
    "    scheduler = sched(optimizer) if sched else None\n",
    "    history = []\n",
    "    # Train the model\n",
    "    while epoch < epochs:\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        model.train()\n",
    "        for x, y in DataLoader(trainset, batch_size=32, shuffle=True):\n",
    "            x: torch.Tensor = x.view(-1, model.input_size).to(device)\n",
    "            y: torch.Tensor = y.to(device=device, dtype=torch.float32)\n",
    "            loss_all, outputs = forward_backward(\n",
    "                optimizer, criterion, wd_reg, model, y, batch_norm, drop_out)\n",
    "            # Learning rate adjustment\n",
    "            if pre_loss:\n",
    "                while pre_loss <= loss_all.item():\n",
    "                    # learning rate vanishing\n",
    "                    if learning_rate < min_lr:\n",
    "                        # return history\n",
    "                        learning_rate = init_lr\n",
    "                        optimizer = opt(model.storage[-1], lr=learning_rate)\n",
    "                        loss_all, outputs = forward_backward(\n",
    "                            optimizer, criterion, wd_reg, model, y, batch_norm, drop_out)\n",
    "                        # raise ValueError(f\"{learning_rate} < {min_lr}\")\n",
    "                        break\n",
    "                    learning_rate *= 0.7\n",
    "                    optimizer = opt(model.storage[-1], lr=learning_rate)\n",
    "                    loss_all, outputs = forward_backward(\n",
    "                        optimizer, criterion, wd_reg, model, y, batch_norm, drop_out)\n",
    "                learning_rate *= 1.2\n",
    "                pre_loss = loss_all.item()\n",
    "            train_loss += loss_all.item() * x.size(0)\n",
    "            model.storage.append(list(model.parameters()))\n",
    "        train_loss /= len(trainset)\n",
    "        # Validate the model\n",
    "        val_loss = validate(\n",
    "            model=model, device=device, valset=valset, criterion=criterion)\n",
    "        # Log statics\n",
    "        history.append((train_loss, val_loss))\n",
    "        # Stopping criteria\n",
    "        if learning_goal > train_loss:\n",
    "            return history\n",
    "        # Update loop\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        epoch += 1\n",
    "    # restore model\n",
    "    model.if_BN = origin_if_BN\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model: TwoLayerNetwork, device: str, testset: Dataset[torch.Tensor]) -> float:\n",
    "    \"\"\"return accuracy\"\"\"\n",
    "    return validate(model=model, device=device, valset=testset, criterion=nn.MSELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogizing(model: TwoLayerNetwork, device: str, trainset: Dataset[torch.Tensor], learning_goal: float, criterion: nn.modules.loss._Loss):\n",
    "    x = torch.stack([x for x, _ in trainset]\n",
    "                    ).view(-1, model.input_size).to(\"cpu\")\n",
    "    y = torch.Tensor([y for _, y in trainset]).to(\"cpu\")\n",
    "    total_amount = len(x)\n",
    "    # get wrong correct indices\n",
    "    new_fc1_w = model.fc1.weight.data.to(device)\n",
    "    new_fc1_b = model.fc1.bias.data.to(device)\n",
    "    new_fc2_w = model.fc2.weight.data.to(device)\n",
    "    relu = nn.ReLU()\n",
    "    logits: torch.Tensor = x\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = relu(x.to(device) @ new_fc1_w.T + new_fc1_b\n",
    "                       ) @ new_fc2_w.T + model.fc2.bias.data.to(device)\n",
    "        logits, predicted = torch.max(outputs.data, 1)\n",
    "        success_condition = predicted == y.to(device)\n",
    "        wrong_indices = torch.nonzero(success_condition != True).to(\"cpu\")\n",
    "    #\n",
    "    init_func: Callable[[torch.Tensor], torch.Tensor] = lambda x: nn.init.xavier_uniform_(\n",
    "        tensor=x) if len(x.shape) > 1 else x\n",
    "    wrong_pointer = len(wrong_indices)\n",
    "    train_correct: int = 0\n",
    "    loss: float = float(\"inf\")\n",
    "    history: List[Tuple[float, float]] = []\n",
    "    while wrong_pointer > 0 and loss > learning_goal:\n",
    "        fc1 = nn.Linear(len(x[0]), 3).to(device)\n",
    "        fc2 = nn.Linear(3, product(model.fc2.bias.size()),\n",
    "                        False).to(device)\n",
    "        # with torch.no_grad():\n",
    "        target = torch.zeros(*torch.Size((total_amount,)))\n",
    "        wrong_pointer -= 1\n",
    "        pointer = wrong_indices[wrong_pointer]\n",
    "        catagory = int(y[pointer])\n",
    "        target[pointer] = catagory\n",
    "        target = target.to(device)\n",
    "        fc2.weight.data[:, :] = 0\n",
    "        fc2.weight.data[catagory, 0] = -2\n",
    "        fc2.weight.data[catagory, 1] = 1\n",
    "        fc2.weight.data[catagory, 2] = 1\n",
    "        delta = 0\n",
    "        intercept = 0\n",
    "        nonz = x\n",
    "        # randomly generate hyperplane which only contain the target x\n",
    "        while nonz.size() != (1, 2) or nonz.tolist()[0][0] != pointer:\n",
    "            for p in fc1.parameters():\n",
    "                init_func(p)\n",
    "            distances = x.to(device) @ fc1.weight.data[0].T\n",
    "            intercept = distances[pointer]\n",
    "            distances -= intercept\n",
    "            # get the shortest distance of other x to hyperplane\n",
    "            if (delta := torch.min(torch.abs(torch.cat(\n",
    "                    (distances[:pointer], distances[pointer + 1:])\n",
    "            )))) == 0:\n",
    "                continue\n",
    "            fc1.bias.data[1] = -intercept + (delta / 2)\n",
    "            fc1.bias.data[2] = -intercept - (delta / 2)\n",
    "            # check if delta too small for float32(default)\n",
    "            if fc1.bias.data[1] == fc1.bias.data[2]:\n",
    "                continue\n",
    "            fc1.bias.data[0] = -intercept\n",
    "            fc1.weight.data[1:] = fc1.weight.data[0]\n",
    "            outputs = relu(x.to(device) @ fc1.weight.data.T + fc1.bias.data\n",
    "                           ) @ fc2.weight.data.T\n",
    "            nonz = torch.nonzero(outputs)\n",
    "        # adjust weight in order to make the output of correct category greater than the others\n",
    "        fc2.weight.data *= logits[pointer].item(\n",
    "        ) / outputs[pointer].sum() + 1\n",
    "        new_fc1_w = torch.cat((new_fc1_w, fc1.weight.data)).to(device)\n",
    "        new_fc1_b = torch.cat((new_fc1_b, fc1.bias.data)).to(device)\n",
    "        new_fc2_w = torch.cat((new_fc2_w, fc2.weight.data), 1).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = relu(x.to(device) @ new_fc1_w.T + new_fc1_b\n",
    "                           ) @ new_fc2_w.T + model.fc2.bias.data.to(device)\n",
    "            loss = criterion(\n",
    "                outputs, y.to(device=device, dtype=torch.long)).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_correct = (predicted == y.to(device)).sum().item()\n",
    "        history.append((loss, train_correct / total_amount))\n",
    "    # construct new model\n",
    "    new_model = TwoLayerNetwork(model.input_size, len(new_fc1_b), product(\n",
    "        model.fc2.bias.size()), lambda _: _, lambda: model.active_func, model.do.p, model.if_BN)\n",
    "    for name, param in model.named_parameters():\n",
    "        layer_name, variable_type = name.split(\".\")\n",
    "        if layer_name == \"fc1\":\n",
    "            setattr(getattr(getattr(new_model, layer_name), variable_type),\n",
    "                    \"data\", eval(f\"new_{layer_name}_{variable_type[0]}\"))\n",
    "        elif layer_name == \"fc2\":\n",
    "            if variable_type == \"weight\":\n",
    "                setattr(getattr(getattr(new_model, layer_name), variable_type),\n",
    "                        \"data\", eval(f\"new_{layer_name}_{variable_type[0]}\"))\n",
    "            elif variable_type == \"bias\":\n",
    "                new_model.fc2.bias.data[:] = model.fc2.bias.data[:]\n",
    "            else:\n",
    "                pass\n",
    "                setattr(getattr(new_model, layer_name), variable_type, param)\n",
    "        else:\n",
    "            setattr(getattr(new_model, layer_name), variable_type, param)\n",
    "    return new_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2628 751 376\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available(\n",
    ") else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "trainset, valset, testset, input_size = getCustomizedData()\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 32\n",
    "init: Callable[[torch.Tensor], torch.Tensor] = lambda x: nn.init.xavier_uniform_(\n",
    "    tensor=x) if len(x.shape) > 1 else x\n",
    "active = nn.ReLU\n",
    "model = TwoLayerNetwork(input_size, hidden_size, 1,\n",
    "                        init, active, 0., False).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.050912660407893204\n",
      "[(0.014791576618639132, 6.024629773929239e-05), (0.0017932431949189151, 6.103930424340194e-05), (0.0017934633336346058, 6.0895479822473365e-05), (0.0017929784528332758, 6.084292637991574e-05), (0.0017931283259540343, 6.11068695996294e-05), (0.0017926669374811414, 6.116548242771654e-05), (0.0017922889606749337, 6.085791536608398e-05), (0.0017926629256709411, 6.091984465509621e-05), (0.0017921632546307787, 6.0715429585719404e-05), (0.0017918162058494543, 6.112065367365303e-05), (0.001791715381498331, 6.033067929166426e-05), (0.0017916151507413143, 6.060882911411456e-05), (0.0017914815631849604, 6.0591439284765894e-05), (0.0017912373828447936, 6.0565408475837485e-05), (0.0017910158234014767, 6.074030030569615e-05), (0.001790999388463988, 6.066077001017398e-05), (0.0017906635226412496, 6.058076548401096e-05), (0.001790369498292537, 6.103196702144957e-05), (0.001790301861876517, 6.081939027014563e-05), (0.0017903644628425894, 5.97822542432367e-05)]\n",
      "5.315698024091073e-05\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 64\n",
    "epochs = 20\n",
    "init: Callable[[torch.Tensor], torch.Tensor] = lambda x: nn.init.xavier_uniform_(\n",
    "    tensor=x) if len(x.shape) > 1 else x\n",
    "active = nn.ReLU\n",
    "model = TwoLayerNetwork(input_size, hidden_size,\n",
    "                        1, init, active, 0., True)\n",
    "optimize = optim.SGD\n",
    "schedule = None\n",
    "learning_rate = 0.001\n",
    "min_lr = learning_rate * 1e-5\n",
    "l2_reg = L2_Regularization(0.0001)\n",
    "baseline = test(model, device, testset)\n",
    "learning_goal = baseline * 0.02\n",
    "history = train(model, optimize, device, epochs, learning_rate,\n",
    "                trainset, valset, criterion, schedule, l2_reg, learning_goal, min_lr, True, False, 0.)\n",
    "result = test(model, device, testset)\n",
    "print(baseline, history, result, sep=\"\\n\")\n",
    "model_path = r\"./data/final\"\n",
    "torch.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

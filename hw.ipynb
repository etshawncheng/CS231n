{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW Requirement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Implement the code for the 2-layer neural networks in CS231n \n",
    "2021 version with PyTorch (or TensorFlow). \n",
    "\n",
    "• Once you have the code (regardless of which framework you \n",
    "choose above), you will apply your own data.  The training and test \n",
    "dataset is 80%:20%.\n",
    "\n",
    "• You need to run the code with the following hyperparameter \n",
    "settings:\n",
    "\n",
    "✓ Activation function: tanh, ReLU\n",
    "\n",
    "✓ Data preprocessing\n",
    "\n",
    "✓ Initial weights: small random number, Xavier or Kaiming/MSRA \n",
    "Initialization\n",
    "\n",
    "✓ Loss function: without or with the regularization term \n",
    "(L2), λ = \n",
    "0.001 or 0.0001\n",
    "$$ E(w) = \\frac{1}{N}\\sum^{N}_{c=1}[𝑓(X^c, w) −y^c]^2 \n",
    " + \\lambda[\\sum^{p}_{i=0}(w^{o}_{i})^2\n",
    " + \\sum_{i=1}^{p}\\sum_{j=0}^{m}(w_{ij}^H)^2]\n",
    "$$\n",
    "✓ Optimizer: gradient descent, Momentum, Adam\n",
    "\n",
    "✓ Learning epochs: 100, 200, 300\n",
    "\n",
    "✓ Amount of hidden nodes: 5, 8, 11\n",
    "\n",
    "✓ Learning rate decay schedule: none and cosine\n",
    "\n",
    "✓ Ensembles: top 3 models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VS_Workplace\\NN\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim, Generator\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sys import stdout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Callable, Type\n",
    "from operator import mul\n",
    "\n",
    "def product(nums: Iterable[Type], func: Callable[[Type, Type], Type] = mul):\n",
    "    def _product(nums):\n",
    "        nonlocal func\n",
    "        if len(nums) == 1:\n",
    "            return nums[0]\n",
    "        return func(nums[-1], _product(nums[:-1]))\n",
    "    try:\n",
    "        return _product(nums)\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVES = {\n",
    "    \"relu\": nn.ReLU,\n",
    "    \"tanh\": nn.Tanh\n",
    "}\n",
    "INIT_FUNCS = {\n",
    "    \"small_random\": lambda x: nn.init.normal_(tensor=x, mean=0, std=0.01),\n",
    "    \"xavier\": lambda x: nn.init.xavier_uniform_(tensor=x) if len(x.shape) > 1 else None,\n",
    "    \"kaiming\": lambda x: nn.init.kaiming_uniform_(tensor=x, nonlinearity='relu') if len(x.shape) > 1 else None\n",
    "}\n",
    "OPTIM_FUNCS = {\n",
    "    \"sgd\": optim.SGD,\n",
    "    \"momentum\": lambda param, lr, weight_decay: optim.SGD(params=param, lr=lr, momentum=0.9, weight_decay=weight_decay),\n",
    "    \"adam\": optim.Adam\n",
    "}\n",
    "SCHEDULERS = {\n",
    "    \"cos\": lambda opt: torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=opt, T_max=200)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "class TwoLayerNetwork(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_classes: int, init_method:Callable, active_func:nn.modules.module.Module) -> None:\n",
    "        super(TwoLayerNetwork, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size= hidden_size\n",
    "        ## first layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        ## activation\n",
    "        self.active_func = active_func()\n",
    "        ## initialize\n",
    "        for param in self.parameters():\n",
    "            init_method(param)\n",
    "        ## second layer\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.active_func(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: TwoLayerNetwork, opt: nn.Module, device: str, epochs: int, learning_rate: float, trainloader: DataLoader, valloader: DataLoader, criterion: nn.modules.loss._Loss, sched: optim.lr_scheduler._LRScheduler, weight_decay:float):\n",
    "    model.to(device)\n",
    "    optimizer = opt(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = sched(optimizer) if sched else None\n",
    "    if epochs < 1:\n",
    "        raise ValueError(\"Invalid epoch!!\")\n",
    "    else:\n",
    "        epochs = int(epochs)\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        model.train()\n",
    "        for X, y in trainloader:\n",
    "            X = X.view(-1, model.input_size).to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_correct += (predicted == y).sum().item()\n",
    "        train_loss /= len(trainloader.dataset)\n",
    "        train_accuracy = 100. * train_correct / len(trainloader.dataset)\n",
    "\n",
    "        # Validate the model\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, y in valloader:\n",
    "                X = X.view(-1, model.input_size).to(device)\n",
    "                y = y.to(device)\n",
    "                outputs = model(X)\n",
    "                loss = criterion(outputs, y)\n",
    "                val_loss += loss.item() * X.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_correct += (predicted == y).sum().item()\n",
    "            val_loss /= len(valloader.dataset)\n",
    "            val_accuracy = 100. * val_correct / len(valloader.dataset)\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        # Print epoch statistics\n",
    "        stdout.write('Epoch [{}/{}], Train Loss: {:.4f}, Train Accuracy: {:.2f}%, Val Loss: {:.4f}, Val Accuracy: {:.2f}%\\n'\n",
    "              .format(epoch+1, epochs, train_loss, train_accuracy, val_loss, val_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model:nn.Module, device:str, testloader:DataLoader):\n",
    "    val_correct = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in testloader:\n",
    "            X = X.view(-1, model.input_size).to(device)\n",
    "            y = y.to(device)\n",
    "            outputs = model(X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_correct += (predicted == y).sum().item()\n",
    "        val_accuracy = 100. * val_correct / len(testloader.dataset)\n",
    "        stdout.write('Val Accuracy: {:.2f}%\\n'.format(val_accuracy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pytorch dataset\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "def getPytorchData():\n",
    "    # preprocess: flatten, normalize, drop 90%, split\n",
    "    transform = transforms.transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    trainset = datasets.FashionMNIST(\n",
    "        root=\"./data/\", train=True, download=False, transform=transform)\n",
    "    datum_size = product(trainset[0][0].size())\n",
    "    class_amount = len(trainset.classes)\n",
    "    testset = datasets.FashionMNIST(\n",
    "        root=\"./data/\", train=False, download=False, transform=transform)\n",
    "    # Split the training set into training and validation sets\n",
    "    train_count = int(0.08 * len(trainset))\n",
    "    valid_count = int(0.02 * len(trainset))\n",
    "    print(train_count, valid_count, len(testset))\n",
    "    trainset, valset, _ = random_split(\n",
    "        trainset, (train_count, valid_count, len(trainset)-train_count-valid_count), Generator().manual_seed(42))\n",
    "    # Create data loaders to load the data in batches\n",
    "    trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "    valloader = DataLoader(valset, batch_size=32, shuffle=True)\n",
    "    testloader = DataLoader(testset, batch_size=32, shuffle=True)\n",
    "    return trainloader, valloader, testloader, datum_size, class_amount\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### customized pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "class HotelReservationDataset(Dataset):\n",
    "    \"\"\"Hotel Reservation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        # 19\n",
    "        reservations = pd.read_csv(csv_path)\n",
    "        # 5\n",
    "        for col in map(lambda x: x[0], filter(lambda x:x[1]==\"O\", reservations.dtypes.items())):\n",
    "            d = dict((j, i) for i, j in enumerate(reservations[col].value_counts().index))\n",
    "            setattr(self, f\"labels_of_{col}\", d.keys())\n",
    "            reservations[col]=reservations[col].map(d.__getitem__)\n",
    "        # 17(drop id)\n",
    "        self.feature = torch.from_numpy(reservations.iloc[:, 1:-1].to_numpy(dtype=np.float32))\n",
    "        # two status\n",
    "        self.booking_status = torch.reshape(torch.tensor(reservations.iloc[:, -1:].to_numpy()), shape=(len(self.feature),))\n",
    "        self.classes = list(getattr(self, f\"labels_of_{reservations.columns[-1]}\"))\n",
    "    def __len__(self):\n",
    "        return len(self.booking_status)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self.feature[idx], self.booking_status[idx]\n",
    "\n",
    "# kaggle: ahsan81/hotel-reservations-classification-dataset\n",
    "def getCustomizedData():\n",
    "    # preprocess\n",
    "    dataset = HotelReservationDataset(\n",
    "        csv_path=r\"D:\\dataset\\archive\\Hotel Reservations.csv\")\n",
    "    class_amount = len(dataset.classes)\n",
    "    # train test split\n",
    "    train_count = int(0.7 * len(dataset))\n",
    "    valid_count = int(0.2 * len(dataset))\n",
    "    test_count = len(dataset) - train_count - valid_count\n",
    "    print(train_count, valid_count, test_count)\n",
    "    trainset, valset, testset = random_split(\n",
    "        dataset, (train_count, valid_count, test_count), Generator().manual_seed(42))\n",
    "    datum_size = product(trainset[0][0].size())\n",
    "    # set loaders\n",
    "    trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "    valloader = DataLoader(valset, batch_size=32, shuffle=True)\n",
    "    testloader = DataLoader(testset, batch_size=32, shuffle=True)\n",
    "    return trainloader, valloader, testloader, datum_size, class_amount\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data(zipped csv) from kaggle with username and apikey\n",
    "import os\n",
    "import json\n",
    "with open(\"kaggle.json\", \"r\") as j:\n",
    "    for (k, v) in json.load(j).items():\n",
    "        os.environ[k] = v\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "# https://www.kaggle.com/datasets/uciml/iris/download?datasetVersionNumber=2\n",
    "# owner/datasetname\n",
    "api.dataset_download_files('uciml/iris', path=\"./data/\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 100 small_random relu sgd cos\n",
      "5 100 small_random relu momentum cos\n",
      "5 100 small_random relu adam cos\n",
      "5 100 small_random tanh sgd cos\n",
      "5 100 small_random tanh momentum cos\n",
      "5 100 small_random tanh adam cos\n",
      "5 100 xavier relu sgd cos\n",
      "5 100 xavier relu momentum cos\n",
      "5 100 xavier relu adam cos\n",
      "5 100 xavier tanh sgd cos\n",
      "5 100 xavier tanh momentum cos\n",
      "5 100 xavier tanh adam cos\n",
      "5 100 kaiming relu sgd cos\n",
      "5 100 kaiming relu momentum cos\n",
      "5 100 kaiming relu adam cos\n",
      "5 100 kaiming tanh sgd cos\n",
      "5 100 kaiming tanh momentum cos\n",
      "5 100 kaiming tanh adam cos\n",
      "5 200 small_random relu sgd cos\n",
      "5 200 small_random relu momentum cos\n",
      "5 200 small_random relu adam cos\n",
      "5 200 small_random tanh sgd cos\n",
      "5 200 small_random tanh momentum cos\n",
      "5 200 small_random tanh adam cos\n",
      "5 200 xavier relu sgd cos\n",
      "5 200 xavier relu momentum cos\n",
      "5 200 xavier relu adam cos\n",
      "5 200 xavier tanh sgd cos\n",
      "5 200 xavier tanh momentum cos\n",
      "5 200 xavier tanh adam cos\n",
      "5 200 kaiming relu sgd cos\n",
      "5 200 kaiming relu momentum cos\n",
      "5 200 kaiming relu adam cos\n",
      "5 200 kaiming tanh sgd cos\n",
      "5 200 kaiming tanh momentum cos\n",
      "5 200 kaiming tanh adam cos\n",
      "5 300 small_random relu sgd cos\n",
      "5 300 small_random relu momentum cos\n",
      "5 300 small_random relu adam cos\n",
      "5 300 small_random tanh sgd cos\n",
      "5 300 small_random tanh momentum cos\n",
      "5 300 small_random tanh adam cos\n",
      "5 300 xavier relu sgd cos\n",
      "5 300 xavier relu momentum cos\n",
      "5 300 xavier relu adam cos\n",
      "5 300 xavier tanh sgd cos\n",
      "5 300 xavier tanh momentum cos\n",
      "5 300 xavier tanh adam cos\n",
      "5 300 kaiming relu sgd cos\n",
      "5 300 kaiming relu momentum cos\n",
      "5 300 kaiming relu adam cos\n",
      "5 300 kaiming tanh sgd cos\n",
      "5 300 kaiming tanh momentum cos\n",
      "5 300 kaiming tanh adam cos\n",
      "8 100 small_random relu sgd cos\n",
      "8 100 small_random relu momentum cos\n",
      "8 100 small_random relu adam cos\n",
      "8 100 small_random tanh sgd cos\n",
      "8 100 small_random tanh momentum cos\n",
      "8 100 small_random tanh adam cos\n",
      "8 100 xavier relu sgd cos\n",
      "8 100 xavier relu momentum cos\n",
      "8 100 xavier relu adam cos\n",
      "8 100 xavier tanh sgd cos\n",
      "8 100 xavier tanh momentum cos\n",
      "8 100 xavier tanh adam cos\n",
      "8 100 kaiming relu sgd cos\n",
      "8 100 kaiming relu momentum cos\n",
      "8 100 kaiming relu adam cos\n",
      "8 100 kaiming tanh sgd cos\n",
      "8 100 kaiming tanh momentum cos\n",
      "8 100 kaiming tanh adam cos\n",
      "8 200 small_random relu sgd cos\n",
      "8 200 small_random relu momentum cos\n",
      "8 200 small_random relu adam cos\n",
      "8 200 small_random tanh sgd cos\n",
      "8 200 small_random tanh momentum cos\n",
      "8 200 small_random tanh adam cos\n",
      "8 200 xavier relu sgd cos\n",
      "8 200 xavier relu momentum cos\n",
      "8 200 xavier relu adam cos\n",
      "8 200 xavier tanh sgd cos\n",
      "8 200 xavier tanh momentum cos\n",
      "8 200 xavier tanh adam cos\n",
      "8 200 kaiming relu sgd cos\n",
      "8 200 kaiming relu momentum cos\n",
      "8 200 kaiming relu adam cos\n",
      "8 200 kaiming tanh sgd cos\n",
      "8 200 kaiming tanh momentum cos\n",
      "8 200 kaiming tanh adam cos\n",
      "8 300 small_random relu sgd cos\n",
      "8 300 small_random relu momentum cos\n",
      "8 300 small_random relu adam cos\n",
      "8 300 small_random tanh sgd cos\n",
      "8 300 small_random tanh momentum cos\n",
      "8 300 small_random tanh adam cos\n",
      "8 300 xavier relu sgd cos\n",
      "8 300 xavier relu momentum cos\n",
      "8 300 xavier relu adam cos\n",
      "8 300 xavier tanh sgd cos\n",
      "8 300 xavier tanh momentum cos\n",
      "8 300 xavier tanh adam cos\n",
      "8 300 kaiming relu sgd cos\n",
      "8 300 kaiming relu momentum cos\n",
      "8 300 kaiming relu adam cos\n",
      "8 300 kaiming tanh sgd cos\n",
      "8 300 kaiming tanh momentum cos\n",
      "8 300 kaiming tanh adam cos\n",
      "11 100 small_random relu sgd cos\n",
      "11 100 small_random relu momentum cos\n",
      "11 100 small_random relu adam cos\n",
      "11 100 small_random tanh sgd cos\n",
      "11 100 small_random tanh momentum cos\n",
      "11 100 small_random tanh adam cos\n",
      "11 100 xavier relu sgd cos\n",
      "11 100 xavier relu momentum cos\n",
      "11 100 xavier relu adam cos\n",
      "11 100 xavier tanh sgd cos\n",
      "11 100 xavier tanh momentum cos\n",
      "11 100 xavier tanh adam cos\n",
      "11 100 kaiming relu sgd cos\n",
      "11 100 kaiming relu momentum cos\n",
      "11 100 kaiming relu adam cos\n",
      "11 100 kaiming tanh sgd cos\n",
      "11 100 kaiming tanh momentum cos\n",
      "11 100 kaiming tanh adam cos\n",
      "11 200 small_random relu sgd cos\n",
      "11 200 small_random relu momentum cos\n",
      "11 200 small_random relu adam cos\n",
      "11 200 small_random tanh sgd cos\n",
      "11 200 small_random tanh momentum cos\n",
      "11 200 small_random tanh adam cos\n",
      "11 200 xavier relu sgd cos\n",
      "11 200 xavier relu momentum cos\n",
      "11 200 xavier relu adam cos\n",
      "11 200 xavier tanh sgd cos\n",
      "11 200 xavier tanh momentum cos\n",
      "11 200 xavier tanh adam cos\n",
      "11 200 kaiming relu sgd cos\n",
      "11 200 kaiming relu momentum cos\n",
      "11 200 kaiming relu adam cos\n",
      "11 200 kaiming tanh sgd cos\n",
      "11 200 kaiming tanh momentum cos\n",
      "11 200 kaiming tanh adam cos\n",
      "11 300 small_random relu sgd cos\n",
      "11 300 small_random relu momentum cos\n",
      "11 300 small_random relu adam cos\n",
      "11 300 small_random tanh sgd cos\n",
      "11 300 small_random tanh momentum cos\n",
      "11 300 small_random tanh adam cos\n",
      "11 300 xavier relu sgd cos\n",
      "11 300 xavier relu momentum cos\n",
      "11 300 xavier relu adam cos\n",
      "11 300 xavier tanh sgd cos\n",
      "11 300 xavier tanh momentum cos\n",
      "11 300 xavier tanh adam cos\n",
      "11 300 kaiming relu sgd cos\n",
      "11 300 kaiming relu momentum cos\n",
      "11 300 kaiming relu adam cos\n",
      "11 300 kaiming tanh sgd cos\n",
      "11 300 kaiming tanh momentum cos\n",
      "11 300 kaiming tanh adam cos\n"
     ]
    }
   ],
   "source": [
    "def training_schedule():\n",
    "    stdout = open(\"./result/hw1.txt\", \"w\")\n",
    "    # processor\n",
    "    device = \"cuda\" if torch.cuda.is_available(\n",
    "    ) else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    # hyper parameters\n",
    "    trainloader, valloader, testloader, input_size, output_size = getPytorchData()\n",
    "    learning_rate = 0.001\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # ✓ Amount of hidden nodes: 5, 8, 11\n",
    "    for hidden_size in (5, 8, 11):\n",
    "        # ✓ Learning epochs: 100, 200, 300\n",
    "        for epochs in (100, 200, 300):\n",
    "            # Create model, optimizer, scheduler\n",
    "            for (init, method) in INIT_FUNCS.items():\n",
    "                for (active, func) in ACTIVES.items():\n",
    "                    # ✓ Activation function: tanh, ReLU\n",
    "                    # ✓ Initial weights: small random number, Xavier or Kaiming/MSRA Initialization\n",
    "                    model = TwoLayerNetwork(input_size, hidden_size, output_size,\n",
    "                                            init_method=method, active_func=func).to(device)\n",
    "                    # ✓ Optimizer: gradient descent, Momentum, Adam\n",
    "                    for (optimize, optm) in OPTIM_FUNCS.items():\n",
    "                        # ✓ Learning rate decay schedule: none and cosine\n",
    "                        for (schedule, schd) in SCHEDULERS.items():\n",
    "                            # ✓ Loss function: without or with L2, λ = 0.001 or 0.0001\n",
    "                            for weight_decay in (0.0, 0.001, 0.0001):\n",
    "                                print(hidden_size, epochs, init, active,\n",
    "                                      optimize, schedule, weight_decay,  \"start\")\n",
    "                                train(model=model, opt=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "                                      trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "                                test(model=model, device=device,\n",
    "                                     testloader=testloader)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800 1200 10000\n"
     ]
    }
   ],
   "source": [
    "# processor\n",
    "device = \"cuda\" if torch.cuda.is_available(\n",
    ") else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "# hyper parameters\n",
    "trainloader, valloader, testloader, input_size, output_size = getPytorchData()\n",
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "hidden_size = 5\n",
    "epochs = 100\n",
    "init = \"small_random\"\n",
    "method = INIT_FUNCS[init]\n",
    "active = \"relu\"\n",
    "func = ACTIVES[active]\n",
    "optimize = \"sgd\"\n",
    "optm = OPTIM_FUNCS[optimize]\n",
    "schedule = None\n",
    "schd = schedule\n",
    "weight_decay = 0.0\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _hidden_size in (5, 8, 11):\n",
    "    counter += 1\n",
    "    model = TwoLayerNetwork(input_size=input_size, hidden_size=_hidden_size,\n",
    "                            num_classes=output_size, init_method=method, active_func=func)\n",
    "    with open(f\"./data/{counter}.txt\", \"w+\") as f:\n",
    "        stdout = f\n",
    "        stdout.write(f\"{counter}: {_hidden_size}, {epochs}, {init}, {active}, {optimize}, {schedule}, {weight_decay}\\n\")\n",
    "        train(model=model, opt=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "              trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "        test(model=model, device=device, testloader=testloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✓ Learning epochs: 100, 200, 300\n",
    "for _epochs in (100, 200, 300):\n",
    "    counter += 1\n",
    "    model = TwoLayerNetwork(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_classes=output_size, init_method=method, active_func=func)\n",
    "    with open(f\"./data/{counter}.txt\", \"w+\") as f:\n",
    "        stdout = f\n",
    "        stdout.write(\n",
    "            f\"{counter}: {hidden_size}, {_epochs}, {init}, {active}, {optimize}, {schedule}, {weight_decay}\\n\")\n",
    "        train(model=model, opt=optm, device=device, epochs=_epochs, learning_rate=learning_rate,\n",
    "              trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "        test(model=model, device=device, testloader=testloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✓ Initial weights: small random number, Xavier or Kaiming/MSRA Initialization\n",
    "for (_init, _method) in INIT_FUNCS.items():\n",
    "    counter += 1\n",
    "    model = TwoLayerNetwork(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_classes=output_size, init_method=_method, active_func=func)\n",
    "    with open(f\"./data/{counter}.txt\", \"w+\") as f:\n",
    "        stdout = f\n",
    "        stdout.write(\n",
    "            f\"{counter}: {hidden_size}, {epochs}, {_init}, {active}, {optimize}, {schedule}, {weight_decay}\\n\")\n",
    "        train(model=model, opt=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "              trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "        test(model=model, device=device, testloader=testloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✓ Activation function: tanh, ReLU\n",
    "for (_active, _func) in ACTIVES.items():\n",
    "    counter += 1\n",
    "    model = TwoLayerNetwork(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_classes=output_size, init_method=method, active_func=_func)\n",
    "    with open(f\"./data/{counter}.txt\", \"w+\") as f:\n",
    "        stdout = f\n",
    "        stdout.write(f\"{counter}: {hidden_size}, {epochs}, {init}, {_active}, {optimize}, {schedule}, {weight_decay}\\n\")\n",
    "        train(model=model, opt=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "              trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "        test(model=model, device=device, testloader=testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✓ Optimizer: gradient descent, Momentum, Adam\n",
    "for (_optimize, _optm) in OPTIM_FUNCS.items():\n",
    "    counter += 1\n",
    "    model = TwoLayerNetwork(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_classes=output_size, init_method=method, active_func=func)\n",
    "    with open(f\"./data/{counter}.txt\", \"w+\") as f:\n",
    "        stdout = f\n",
    "        stdout.write(f\"{counter}: {hidden_size}, {epochs}, {init}, {active}, {_optimize}, {schedule}, {weight_decay}\\n\")\n",
    "        train(model=model, opt=_optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "              trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "        test(model=model, device=device, testloader=testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✓ Learning rate decay schedule: none and cosine\n",
    "for (_schedule, _schd) in SCHEDULERS.items():\n",
    "    counter += 1\n",
    "    model = TwoLayerNetwork(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_classes=output_size, init_method=method, active_func=func)\n",
    "    with open(f\"./data/{counter}.txt\", \"w+\") as f:\n",
    "        stdout = f\n",
    "        stdout.write(f\"{counter}: {hidden_size}, {epochs}, {init}, {active}, {optimize}, {_schedule}, {weight_decay}\\n\")\n",
    "        train(model=model, opt=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "              trainloader=trainloader, valloader=valloader, criterion=criterion, sched=_schd, weight_decay=weight_decay)\n",
    "        test(model=model, device=device, testloader=testloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✓ Loss function: without or with L2, λ = 0.001 or 0.0001\n",
    "for _weight_decay in (0.0, 0.001, 0.0001):\n",
    "    counter += 1\n",
    "    model = TwoLayerNetwork(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_classes=output_size, init_method=method, active_func=func)\n",
    "    with open(f\"./data/{counter}.txt\", \"w+\") as f:\n",
    "        stdout = f\n",
    "        stdout.write(f\"{counter}: {hidden_size}, {epochs}, {init}, {active}, {optimize}, {schedule}, {_weight_decay}\\n\")\n",
    "        train(model=model, opt=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "              trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=_weight_decay)\n",
    "        test(model=model, device=device, testloader=testloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 19):\n",
    "    with open(f\"./data/{counter}.txt\") as f:\n",
    "        pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f\"./data/1.txt\", \"r\") as f:\n",
    "    l = list(f.readlines())\n",
    "    l[]\n",
    "    accuracy = l[len(l) - 1][14:-2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"Val Accuracy: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0725737be4be03859ccf648c604bdce2d511d4addba95219b9055f0ea318ae44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1DSSHN_lRd0A_tPBwYBi6zlOd_9N1DBJ3#scrollTo=dpz7yKFTYXPZ)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW Requirement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Rewrite the best code (of the weight-tuning_EB\n",
    "referring to page 14) you have for HW #1 into \n",
    "the code of the weight-tuning_LG. Best means the \n",
    "best hyperparameter setting.\n",
    "\n",
    "\n",
    "• Rewrite the best code (of the weight-tuning_EB\n",
    "referring to page 14) you have for HW #1 into \n",
    "the code of the weight-tuning_EB_LG.\n",
    "\n",
    "\n",
    "• Once you have the code, you will apply the code \n",
    "to learn your dataset for HW #1.\n",
    "\n",
    "\n",
    "• The training and test dataset is 80%/20%.\n",
    "\n",
    "\n",
    "• The performance comparison benchmark is your \n",
    "best weight-tuning_EB."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 6 \\\n",
    "hidden nodes: 11 \\\n",
    "epochs: 300 \\\n",
    "init: xavier \\\n",
    "active: relu \\\n",
    "optimize: sgd \\\n",
    "schedule: None \\\n",
    "weight decay: 0.0\n",
    "\n",
    "Model Accuracy: 82.17% \\\n",
    "Training Time: 639.923 s \\\n",
    "Epoch with hightest Train accuracy: 292, 86.19% \\\n",
    "Epoch with hightest Val accuracy= 249, 83.83%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VS_Workplace\\NN\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim, Generator\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import sys\n",
    "import pandas as pd\n",
    "import cProfile\n",
    "import pstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Callable, Type\n",
    "from operator import mul\n",
    "\n",
    "def product(nums: Iterable[Type], func: Callable[[Type, Type], Type] = mul):\n",
    "    def _product(nums):\n",
    "        nonlocal func\n",
    "        if len(nums) == 1:\n",
    "            return nums[0]\n",
    "        return func(nums[-1], _product(nums[:-1]))\n",
    "    try:\n",
    "        return _product(nums)\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "class TwoLayerNetwork(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_classes: int, init_method:Callable, active_func:nn.modules.module.Module) -> None:\n",
    "        super(TwoLayerNetwork, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size= hidden_size\n",
    "        ## first layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        ## activation\n",
    "        self.active_func = active_func()\n",
    "        ## initialize\n",
    "        for param in self.parameters():\n",
    "            init_method(param)\n",
    "        ## second layer\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.active_func(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: TwoLayerNetwork, opt: nn.Module, device: str, epochs: int, learning_rate: float, trainloader: DataLoader, valloader: DataLoader, criterion: nn.modules.loss._Loss, sched: optim.lr_scheduler._LRScheduler, weight_decay: float, learning_goal: float):\n",
    "    if epochs < 1:\n",
    "        raise ValueError(\"Invalid epoch!!\")\n",
    "    else:\n",
    "        epochs = int(epochs)\n",
    "    model.to(device)\n",
    "    optimizer = opt(model.parameters(), lr=learning_rate,\n",
    "                    weight_decay=weight_decay)\n",
    "    scheduler = sched(optimizer) if sched else None\n",
    "    history = []\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        model.train()\n",
    "        for X, y in trainloader:\n",
    "            X = X.view(-1, model.input_size).to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_correct += (predicted == y).sum().item()\n",
    "        train_loss /= len(trainloader.dataset)\n",
    "        train_accuracy = train_correct / len(trainloader.dataset)\n",
    "\n",
    "        # Validate the model\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, y in valloader:\n",
    "                X = X.view(-1, model.input_size).to(device)\n",
    "                y = y.to(device)\n",
    "                outputs = model(X)\n",
    "                loss = criterion(outputs, y)\n",
    "                val_loss += loss.item() * X.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_correct += (predicted == y).sum().item()\n",
    "            val_loss /= len(valloader.dataset)\n",
    "            val_accuracy = val_correct / len(valloader.dataset)\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        # Print epoch statistics\n",
    "        history.append((train_loss, train_accuracy, val_loss, val_accuracy))\n",
    "        if learning_goal< val_accuracy:\n",
    "            return history\n",
    "        # sys.stdout.write('Epoch [{}/{}], Train Loss: {:.4f}, Train Accuracy: {:.2f}%, Val Loss: {:.4f}, Val Accuracy: {:.2f}%\\n'\n",
    "        #       .format(epoch+1, epochs, train_loss, train_accuracy, val_loss, val_accuracy))\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model:nn.Module, device:str, testloader:DataLoader):\n",
    "    val_correct = 0\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in testloader:\n",
    "            X = X.view(-1, model.input_size).to(device)\n",
    "            y = y.to(device)\n",
    "            outputs = model(X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_correct += (predicted == y).sum().item()\n",
    "        val_accuracy = val_correct / len(testloader.dataset)\n",
    "    return val_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pytorch dataset\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def getPytorchData(train: float = 0.8, remain: float = 0.1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        train: train_amount / total_amount or 1 - valid_amount / total_amount\n",
    "        remain: reduce data amount to save time\n",
    "    \"\"\"\n",
    "    # preprocess: flatten, normalize, drop 90%, split\n",
    "    transform = transforms.transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    if 0 >= train or train >= 1:\n",
    "        raise ValueError()\n",
    "    if 0 > remain or remain > 1:\n",
    "        raise ValueError()\n",
    "    # Split the training set into training and validation sets\n",
    "    trainset = datasets.FashionMNIST(\n",
    "        root=\"./data/\", train=True, download=False, transform=transform)\n",
    "    train_count = int(train * remain * len(trainset))\n",
    "    valid_count = int((1-train) * remain * len(trainset))\n",
    "    if train_count * valid_count == 0:\n",
    "        raise ValueError()\n",
    "    datum_size = product(trainset[0][0].size())\n",
    "    class_amount = len(trainset.classes)\n",
    "    testset = datasets.FashionMNIST(\n",
    "        root=\"./data/\", train=False, download=False, transform=transform)\n",
    "    print(train_count, valid_count, len(testset))\n",
    "    trainset, valset, _ = random_split(\n",
    "        trainset, (train_count, valid_count, len(trainset)-train_count-valid_count), Generator().manual_seed(42))\n",
    "    # Create dataloaders to load the data in batches\n",
    "    trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "    valloader = DataLoader(valset, batch_size=32, shuffle=True)\n",
    "    testloader = DataLoader(testset, batch_size=32, shuffle=True)\n",
    "    return trainloader, valloader, testloader, datum_size, class_amount\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800 1199 10000\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available(\n",
    ") else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "# hyper parameters\n",
    "trainloader, valloader, testloader, input_size, output_size = getPytorchData()\n",
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epoch bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1032\n",
      "[(2.3703084659576414, 0.12458333333333334, 2.2665171641126287, 0.16930775646371976), (2.2073361444473267, 0.19708333333333333, 2.161963452887197, 0.22101751459549623), (2.10985373655955, 0.24083333333333334, 2.070713071648134, 0.2702251876563803), (2.015818354288737, 0.27708333333333335, 1.975842498162073, 0.29608006672226855), (1.9175854190190633, 0.30833333333333335, 1.8757808720498805, 0.3286071726438699), (1.818630797068278, 0.341875, 1.776828143475352, 0.36613844870725604), (1.7220336031913757, 0.37666666666666665, 1.6795025864076973, 0.3969974979149291), (1.627836935520172, 0.40520833333333334, 1.5852018527133551, 0.4278565471226022), (1.538087814648946, 0.440625, 1.4964610224867783, 0.46872393661384487), (1.455324464639028, 0.48270833333333335, 1.4158306419103717, 0.5012510425354462), (1.380937574704488, 0.518125, 1.343318090029216, 0.5371142618849041), (1.314035340944926, 0.5489583333333333, 1.2780349855128679, 0.5613010842368641), (1.253947012424469, 0.5677083333333334, 1.218816505460763, 0.5996663886572143), (1.1998931070168812, 0.5958333333333333, 1.1651815883908498, 0.6171809841534612), (1.1515905598799387, 0.6214583333333333, 1.1175058821621688, 0.6513761467889908), (1.1092946592966715, 0.6427083333333333, 1.075446725537918, 0.6680567139282736), (1.0719945343335469, 0.6545833333333333, 1.039277624895018, 0.6855713094245204), (1.0393581914901733, 0.67125, 1.0064889319843804, 0.6939115929941618), (1.0103740616639456, 0.6783333333333333, 0.9786301701639571, 0.6972477064220184), (0.9847162890434266, 0.6889583333333333, 0.9515056263117914, 0.7055879899916597), (0.9613132842381795, 0.6958333333333333, 0.9284603259183647, 0.7089241034195163), (0.9403103140989939, 0.6995833333333333, 0.907193800078719, 0.7172643869891576), (0.9212579166889191, 0.7052083333333333, 0.8879667411156751, 0.7139282735613011), (0.9037957978248596, 0.70875, 0.8698665087575014, 0.7180984153461217), (0.8876392420132955, 0.711875, 0.854109345027265, 0.725604670558799), (0.8726602061589559, 0.7135416666666666, 0.8392487791700101, 0.7222685571309424), (0.8592239431540172, 0.7152083333333333, 0.8261608931797559, 0.7281067556296914), (0.8461736118793488, 0.7179166666666666, 0.8126033779180876, 0.7272727272727273), (0.8346137273311615, 0.7189583333333334, 0.8020052097557583, 0.7306088407005839), (0.8234916641314824, 0.7195833333333334, 0.7908715499352176, 0.7347789824854045), (0.8131330019235611, 0.721875, 0.7813285950624117, 0.7331109257714762), (0.8034903236230214, 0.7239583333333334, 0.7714380798387567, 0.737281067556297), (0.7946799194812775, 0.724375, 0.7619364449041302, 0.7406171809841534), (0.7858827481667201, 0.724375, 0.7562999465845345, 0.7356130108423686), (0.7781153237819671, 0.7275, 0.7490528715561587, 0.7422852376980817), (0.7702398431301117, 0.7310416666666667, 0.7409683886520857, 0.7489574645537949), (0.7633643980820973, 0.730625, 0.7330316712798627, 0.7464553794829024), (0.7566597201426823, 0.7308333333333333, 0.7272336641085755, 0.7506255212677231), (0.7503072915474573, 0.730625, 0.7212804342628619, 0.7506255212677231), (0.7441652192672094, 0.7360416666666667, 0.7163576312220226, 0.7522935779816514), (0.7384709904591242, 0.73375, 0.7124787819097199, 0.7464553794829024), (0.7325780373811722, 0.7339583333333334, 0.704655689930697, 0.7531276063386155), (0.727799061735471, 0.735, 0.7014105463147263, 0.7547956630525438), (0.7229005475838979, 0.7358333333333333, 0.6976411850180001, 0.7531276063386155), (0.7175284310181935, 0.7414583333333333, 0.6917060624071715, 0.7572977481234362), (0.7133130133152008, 0.7385416666666667, 0.6884173416315863, 0.7547956630525438), (0.708560964067777, 0.74, 0.6847002724491625, 0.7547956630525438), (0.7042996213833491, 0.7402083333333334, 0.6813144787487335, 0.7564637197664721), (0.7002247436841329, 0.7429166666666667, 0.6782271818780621, 0.7564637197664721), (0.6962546956539154, 0.7422916666666667, 0.6744008188549929, 0.7581317764804003), (0.6919685121377309, 0.7441666666666666, 0.6710943191423329, 0.7581317764804003), (0.6887160446246465, 0.7477083333333333, 0.6678833475800928, 0.7547956630525438), (0.6847801367441814, 0.7472916666666667, 0.6637978000974933, 0.762301918265221), (0.681425400575002, 0.7489583333333333, 0.6610195396640481, 0.7597998331943286), (0.677925568819046, 0.749375, 0.6598891131673086, 0.7606338615512928), (0.6746220312515895, 0.7497916666666666, 0.657187008678764, 0.7631359466221852), (0.67152086297671, 0.7502083333333334, 0.6535401468082107, 0.762301918265221), (0.6680065715312957, 0.7541666666666667, 0.653027265940834, 0.7639699749791493), (0.6652383359273275, 0.754375, 0.6486763571877595, 0.762301918265221), (0.6622698785861333, 0.7545833333333334, 0.6474236916064023, 0.7631359466221852), (0.6592019732793172, 0.7560416666666666, 0.642267025242854, 0.7639699749791493), (0.6565247605244319, 0.7560416666666666, 0.6411297585290109, 0.7698081734778982), (0.6537293674548467, 0.7560416666666666, 0.6394610728096027, 0.7689741451209341), (0.6508118629455566, 0.756875, 0.6390864206812001, 0.7698081734778982), (0.6485776166121165, 0.7583333333333333, 0.634347198653758, 0.7706422018348624), (0.6457392408450444, 0.75875, 0.6315259716230397, 0.7714762301918265), (0.6430039288600286, 0.7614583333333333, 0.6298479476106276, 0.7731442869057548), (0.6408896766106288, 0.7610416666666666, 0.6280979056523381, 0.7731442869057548), (0.6382990392049154, 0.7608333333333334, 0.6273146050984507, 0.7731442869057548), (0.6359398720661799, 0.7633333333333333, 0.6242260966726498, 0.7781484570475397), (0.6335881205399831, 0.7635416666666667, 0.6222424957531507, 0.7798165137614679), (0.6312286700805029, 0.7641666666666667, 0.6203013076396462, 0.7814845704753962), (0.6287880750497182, 0.7620833333333333, 0.619403384147434, 0.773978315262719), (0.6269681894779205, 0.7654166666666666, 0.6181279165830286, 0.7748123436196831), (0.6247082298994064, 0.7666666666666667, 0.6172297504964325, 0.7764804003336113), (0.6225827811161677, 0.7654166666666666, 0.6139169207108428, 0.7789824854045038), (0.6203643784920374, 0.768125, 0.6122176989701711, 0.780650542118432), (0.6180589256683985, 0.77125, 0.6105660614468238, 0.7814845704753962), (0.6163067930936813, 0.7704166666666666, 0.6080470618801976, 0.7848206839032527), (0.614070618947347, 0.7708333333333334, 0.6075840449711002, 0.786488740617181), (0.6123263782262802, 0.7720833333333333, 0.6059725280599458, 0.7848206839032527), (0.6101184944311778, 0.7716666666666666, 0.6066206973030529, 0.7781484570475397), (0.6082627979914347, 0.77625, 0.6037267353308012, 0.780650542118432), (0.6061446263392767, 0.7758333333333334, 0.6017057354297113, 0.7831526271893244), (0.6043405320247014, 0.7745833333333333, 0.6006206897917741, 0.7873227689741451), (0.6025142665704092, 0.776875, 0.6003269801744329, 0.7856547122602169), (0.6006264809767405, 0.7754166666666666, 0.5978997925403617, 0.7906588824020017), (0.5988488767544429, 0.778125, 0.5963424531194546, 0.7898248540450375), (0.5971778341134389, 0.7797916666666667, 0.5956395442432121, 0.7881567973311092), (0.5951164106527964, 0.7783333333333333, 0.5944619504385337, 0.7873227689741451), (0.593288413087527, 0.7810416666666666, 0.594776174294342, 0.7839866555462885), (0.5918118600050608, 0.78375, 0.5914731232497571, 0.7873227689741451), (0.5895823262135188, 0.780625, 0.5896738122362609, 0.7973311092577148), (0.5880167625347773, 0.7814583333333334, 0.5882112955778613, 0.7973311092577148), (0.5862209331989289, 0.785625, 0.5870998697394227, 0.7948290241868223), (0.5850184226036071, 0.7820833333333334, 0.587317797469536, 0.7956630525437864), (0.5833052152395248, 0.7860416666666666, 0.5849442366165752, 0.7956630525437864), (0.5815897164742152, 0.78625, 0.5855095075556792, 0.7906588824020017), (0.5798336491982142, 0.7866666666666666, 0.584372617459675, 0.7939949958298582), (0.5785727373758952, 0.7854166666666667, 0.582125698605411, 0.798999165971643), (0.5767451198895772, 0.7864583333333334, 0.5797031284671113, 0.8006672226855713), (0.5753364400068919, 0.7883333333333333, 0.5813336023646061, 0.7948290241868223), (0.5735663960377375, 0.7885416666666667, 0.5791364910703982, 0.7973311092577148), (0.5719466586907704, 0.7895833333333333, 0.577806743559388, 0.8031693077564637), (0.5710429590940476, 0.7902083333333333, 0.5786079794193328, 0.7973311092577148), (0.5695486106475194, 0.7920833333333334, 0.5759600659426896, 0.8015012510425354), (0.5679911426703135, 0.7914583333333334, 0.5751034019389085, 0.7981651376146789), (0.5663686925172806, 0.7925, 0.5737210722194303, 0.8040033361134279), (0.564951849381129, 0.7925, 0.5723535128192567, 0.8023352793994996), (0.5638164302706719, 0.7941666666666667, 0.5717156733742746, 0.804837364470392), (0.5622646805644035, 0.7933333333333333, 0.5709375723189767, 0.8040033361134279), (0.5611505705118179, 0.7927083333333333, 0.5715670971894284, 0.8040033361134279), (0.5598994690179825, 0.79375, 0.5691304865034548, 0.8056713928273561), (0.5579846459627151, 0.795625, 0.5689073920597922, 0.8056713928273561), (0.5564023832480113, 0.7989583333333333, 0.5691103684569121, 0.8056713928273561), (0.5552161107460658, 0.79625, 0.5658813638424655, 0.8098415346121768), (0.5537940386931102, 0.795, 0.5682329786032613, 0.804837364470392), (0.552509605884552, 0.7983333333333333, 0.5673126688194434, 0.8040033361134279), (0.5515380956729253, 0.7979166666666667, 0.5640714308736322, 0.804837364470392), (0.5504049682617187, 0.7972916666666666, 0.5649813763343662, 0.804837364470392), (0.5489064520597458, 0.8004166666666667, 0.563801283534116, 0.8056713928273561), (0.5480248087644577, 0.8014583333333334, 0.5623082781056745, 0.8098415346121768), (0.5464808949828148, 0.8008333333333333, 0.5609973024287952, 0.8056713928273561), (0.5453912454843521, 0.8022916666666666, 0.5597597858227721, 0.810675562969141), (0.5439358208576838, 0.8014583333333334, 0.5590933102483646, 0.8056713928273561), (0.5430138021707535, 0.8022916666666666, 0.5604559407818804, 0.8065054211843202), (0.5419050802787145, 0.8029166666666666, 0.5584599096840674, 0.810675562969141), (0.5404932423432668, 0.8027083333333334, 0.5565641364820606, 0.8065054211843202), (0.539473782380422, 0.805, 0.5580231767082533, 0.8081734778982486), (0.5378767893711726, 0.8041666666666667, 0.5556403223527681, 0.8115095913261051), (0.5372332949439684, 0.8060416666666667, 0.554276386184231, 0.8098415346121768), (0.5359476953744888, 0.80625, 0.5535236335824787, 0.8098415346121768), (0.534942684173584, 0.804375, 0.5533283328990125, 0.8065054211843202), (0.5335995158553124, 0.804375, 0.5558039822908518, 0.8056713928273561), (0.5327132526040077, 0.81, 0.5541589186030492, 0.8065054211843202), (0.53171335875988, 0.8070833333333334, 0.5514013006897943, 0.8115095913261051), (0.5300221502780914, 0.8072916666666666, 0.5526301656990273, 0.8073394495412844), (0.5290691723426183, 0.8060416666666667, 0.549518332966573, 0.8123436196830692), (0.5281569135189056, 0.8095833333333333, 0.552077056369352, 0.810675562969141), (0.5274640191594759, 0.809375, 0.5517742484683689, 0.8098415346121768), (0.5261919703086217, 0.8102083333333333, 0.5498457729468453, 0.8123436196830692), (0.5252225826183955, 0.8102083333333333, 0.5498925288882824, 0.8123436196830692), (0.5237004399299622, 0.80875, 0.5472772425169543, 0.810675562969141), (0.5234097563227018, 0.810625, 0.5475683902282333, 0.8131776480400333), (0.5217861819267273, 0.813125, 0.5447877616908174, 0.8181818181818182), (0.5216370024283727, 0.8108333333333333, 0.5460881996492827, 0.8131776480400333), (0.5199204592903455, 0.8125, 0.5432431543738371, 0.8140116763969975), (0.5194983291625976, 0.8122916666666666, 0.5453858143394444, 0.8165137614678899), (0.5182737378279368, 0.8120833333333334, 0.5446446535983018, 0.8165137614678899), (0.5174582153558731, 0.81125, 0.5444759445030357, 0.8165137614678899), (0.5162916187445322, 0.8127083333333334, 0.542766106188347, 0.8181818181818182), (0.5156013676524163, 0.8133333333333334, 0.5442287180948695, 0.8156797331109258), (0.514618041117986, 0.8139583333333333, 0.5415364943811355, 0.8165137614678899), (0.5138018728295962, 0.8133333333333334, 0.5418558673773138, 0.8165137614678899), (0.5126591576139132, 0.8141666666666667, 0.5418344973722431, 0.817347789824854), (0.5115118236343066, 0.8154166666666667, 0.5405960187154377, 0.8140116763969975), (0.5111482288440069, 0.8141666666666667, 0.5405150355151338, 0.8123436196830692), (0.509553882976373, 0.8141666666666667, 0.5394150296581895, 0.8131776480400333), (0.5091618300477664, 0.814375, 0.5397282677506088, 0.8165137614678899), (0.5084276552001635, 0.8139583333333333, 0.541928420522195, 0.817347789824854), (0.5070393291115761, 0.8139583333333333, 0.5381580371474901, 0.8140116763969975), (0.5064952570199966, 0.8135416666666667, 0.5401216689699188, 0.8156797331109258), (0.5059972521662712, 0.8160416666666667, 0.5398472123488075, 0.8165137614678899), (0.5042481482028961, 0.8170833333333334, 0.5363644158919718, 0.8181818181818182), (0.5040272102753321, 0.8166666666666667, 0.5368184596921525, 0.8140116763969975), (0.5031355111797651, 0.8179166666666666, 0.5353469382086031, 0.8156797331109258), (0.5024709249536197, 0.816875, 0.5353359311098254, 0.817347789824854), (0.5004503027598063, 0.8185416666666666, 0.5421917857081816, 0.8165137614678899), (0.5007380004723867, 0.8202083333333333, 0.5334005220618419, 0.8156797331109258), (0.4999120432138443, 0.81875, 0.5344102253905926, 0.8148457047539617), (0.4990195413430532, 0.8208333333333333, 0.5343404570353638, 0.8165137614678899), (0.4983145993947983, 0.8195833333333333, 0.5336237222179957, 0.8181818181818182), (0.49733913818995157, 0.8183333333333334, 0.5330082300158716, 0.8156797331109258), (0.49651767551898957, 0.8197916666666667, 0.5332266729110673, 0.8165137614678899), (0.49585279196500776, 0.819375, 0.5343485652058199, 0.8181818181818182), (0.4947477671504021, 0.820625, 0.5315302352392246, 0.8140116763969975), (0.49410081227620445, 0.82, 0.533985441480705, 0.817347789824854), (0.4935802571972211, 0.8189583333333333, 0.5313842319467845, 0.8223519599666389), (0.49271302938461303, 0.82375, 0.5305734176353378, 0.8181818181818182), (0.4923401125272115, 0.8197916666666667, 0.5304394085472877, 0.8190158465387823), (0.4910764744877815, 0.820625, 0.5304729659374402, 0.8190158465387823), (0.49054447680711744, 0.823125, 0.52896725303735, 0.817347789824854), (0.4901204405228297, 0.8222916666666666, 0.5293146682789766, 0.8181818181818182), (0.4892069341739019, 0.823125, 0.5286449303817908, 0.8215179316096747), (0.48855942000945407, 0.8222916666666666, 0.5295480468603251, 0.8223519599666389), (0.48778839498758314, 0.82375, 0.5288340773206636, 0.8165137614678899), (0.48679519365231194, 0.8233333333333334, 0.5290574084181304, 0.817347789824854), (0.48622972548007964, 0.8247916666666667, 0.528893122084445, 0.8156797331109258), (0.4856226753195127, 0.8241666666666667, 0.5270296025017682, 0.817347789824854), (0.48504539996385576, 0.821875, 0.5278962451383609, 0.8198498748957465), (0.4841788535316785, 0.8260416666666667, 0.5262118880205894, 0.8190158465387823), (0.48360125641028084, 0.8252083333333333, 0.5305415879099243, 0.817347789824854), (0.4827891725301743, 0.8227083333333334, 0.5265897977242776, 0.823185988323603), (0.4816325624783834, 0.828125, 0.5290890124512275, 0.8190158465387823), (0.48205175002415973, 0.8275, 0.5264342696941128, 0.817347789824854), (0.4805716695388158, 0.8277083333333334, 0.5265622099704599, 0.8190158465387823), (0.48002701659997304, 0.8260416666666667, 0.5244121178922502, 0.8198498748957465), (0.47963541915019353, 0.8277083333333334, 0.5254797119215392, 0.8215179316096747), (0.47905447651942573, 0.8260416666666667, 0.5261343152350043, 0.8190158465387823), (0.4782148526112239, 0.8302083333333333, 0.5248148860883673, 0.817347789824854), (0.4778080950180689, 0.8266666666666667, 0.5250539394768206, 0.8198498748957465), (0.4769717596968015, 0.8258333333333333, 0.5260100657985646, 0.817347789824854), (0.47629186064004897, 0.830625, 0.5254821481557564, 0.8240200166805671), (0.47554177343845366, 0.8289583333333334, 0.5226932250528757, 0.823185988323603), (0.47514331817626954, 0.8291666666666667, 0.5247098672529178, 0.8223519599666389), (0.4738590876261393, 0.8295833333333333, 0.52722407074597, 0.823185988323603), (0.474004341562589, 0.8295833333333333, 0.5235631064984876, 0.8206839032527106), (0.47301775008440017, 0.828125, 0.5257022485223982, 0.8190158465387823), (0.4724594674507777, 0.8316666666666667, 0.5237348001136494, 0.8240200166805671), (0.472133751809597, 0.83125, 0.5222491809633795, 0.8215179316096747), (0.4714715047677358, 0.831875, 0.5228557514836134, 0.823185988323603), (0.471057025094827, 0.8295833333333333, 0.5216275030518691, 0.8240200166805671), (0.4705625833074252, 0.830625, 0.520930734323998, 0.8273561301084237), (0.46950745522975923, 0.83125, 0.5211248425417289, 0.8240200166805671), (0.4685385322570801, 0.8325, 0.5251310530357106, 0.8206839032527106), (0.4684148868918419, 0.830625, 0.5262032215847782, 0.8198498748957465), (0.46777223428090414, 0.8333333333333334, 0.5223419791564432, 0.8240200166805671), (0.4670136390129725, 0.8320833333333333, 0.520732541647029, 0.8223519599666389), (0.46686802794535953, 0.8341666666666666, 0.5191015512173727, 0.8240200166805671), (0.4660738101601601, 0.8341666666666666, 0.5232405395086255, 0.8206839032527106), (0.46553375452756884, 0.8333333333333334, 0.5202548985087544, 0.8248540450375312), (0.4646846879522006, 0.8341666666666666, 0.5207697904984886, 0.8198498748957465), (0.464105042219162, 0.8325, 0.520005922301597, 0.823185988323603), (0.46382547775904337, 0.8354166666666667, 0.5209101401486925, 0.8215179316096747), (0.4632381520668666, 0.8347916666666667, 0.5211285027839225, 0.823185988323603), (0.46256765802701316, 0.834375, 0.5181139355231763, 0.8248540450375312), (0.462299498518308, 0.8347916666666667, 0.5174034869203575, 0.8265221017514596), (0.46160719960927965, 0.8377083333333334, 0.5198899077016975, 0.8281901584653878), (0.461090094546477, 0.8358333333333333, 0.5212880830848287, 0.8223519599666389), (0.46047495474418004, 0.8366666666666667, 0.5192299413124256, 0.8265221017514596), (0.45973832974831264, 0.8364583333333333, 0.5179793315320536, 0.823185988323603), (0.4595866573850314, 0.8354166666666667, 0.5194199267305664, 0.8248540450375312), (0.45905555337667464, 0.8352083333333333, 0.5181932503278301, 0.8273561301084237), (0.458106830616792, 0.8366666666666667, 0.5189174768624453, 0.8240200166805671), (0.45801800866921744, 0.8370833333333333, 0.5207714025083435, 0.823185988323603), (0.4573191319902738, 0.8375, 0.5173218704890569, 0.8281901584653878), (0.4570431155959765, 0.8354166666666667, 0.5189497435808779, 0.8265221017514596), (0.4559033097823461, 0.839375, 0.5227290018585148, 0.8206839032527106), (0.4558920986453692, 0.8383333333333334, 0.5183456533272331, 0.8273561301084237), (0.4552690824866295, 0.8377083333333334, 0.5187791902090332, 0.8248540450375312), (0.45410429904858274, 0.8410416666666667, 0.5191250876574043, 0.8248540450375312), (0.4545141420761744, 0.8366666666666667, 0.5157894702182401, 0.8273561301084237), (0.4536538505057494, 0.8404166666666667, 0.5164846046155845, 0.8240200166805671), (0.45331266790628433, 0.8397916666666667, 0.5164296606165653, 0.8273561301084237), (0.4526015383998553, 0.8395833333333333, 0.5183658154931439, 0.8240200166805671), (0.45224832008282345, 0.8395833333333333, 0.5144144230280646, 0.8281901584653878), (0.4512866692741712, 0.84125, 0.5180298164051905, 0.8240200166805671), (0.45158453077077865, 0.8402083333333333, 0.5156977606799028, 0.8281901584653878), (0.45079242795705793, 0.8391666666666666, 0.5161345930920729, 0.8256880733944955), (0.45025715281565987, 0.84125, 0.5175192903438343, 0.8240200166805671), (0.4496700888375441, 0.8408333333333333, 0.514333113916125, 0.8281901584653878), (0.4494664911429087, 0.8422916666666667, 0.5163892485058437, 0.8248540450375312), (0.4491082638502121, 0.8414583333333333, 0.5167332248850799, 0.8248540450375312), (0.44846577286720274, 0.8427083333333333, 0.5166392473005275, 0.8290241868223519), (0.44781642735004423, 0.8416666666666667, 0.5149107823727825, 0.8290241868223519), (0.4475736461083094, 0.8422916666666667, 0.5135470571072526, 0.8315262718932444), (0.4469354544083277, 0.8439583333333334, 0.5163957450268565, 0.8240200166805671), (0.4460859685142835, 0.843125, 0.5153246756093118, 0.8248540450375312), (0.4460233536362648, 0.8445833333333334, 0.5172194047606519, 0.8256880733944955), (0.4459115798274676, 0.8425, 0.5134956249353585, 0.8290241868223519), (0.44505779494841896, 0.844375, 0.51399123474794, 0.8298582151793161), (0.4445020124316216, 0.8439583333333334, 0.51626012845274, 0.8215179316096747), (0.444412479698658, 0.8433333333333334, 0.5141982039478642, 0.8256880733944955), (0.44361365457375845, 0.8445833333333334, 0.512948394269124, 0.8290241868223519), (0.4432565591732661, 0.84125, 0.514572960570815, 0.8315262718932444), (0.4428977321585019, 0.8445833333333334, 0.5167701541880352, 0.823185988323603), (0.44236824462811153, 0.8447916666666667, 0.5147299460314829, 0.8281901584653878), (0.4415789144237836, 0.845625, 0.5136563983233995, 0.8290241868223519), (0.44173261925578117, 0.8454166666666667, 0.5150650238821763, 0.8281901584653878), (0.44087624698877337, 0.8454166666666667, 0.5139958181910161, 0.8306922435362802), (0.44045413374900816, 0.8445833333333334, 0.5148925900061594, 0.8273561301084237), (0.4401673100392024, 0.8452083333333333, 0.5152866126250186, 0.8290241868223519), (0.43965491771697995, 0.845, 0.5175255086394128, 0.8223519599666389), (0.43932615548372267, 0.8464583333333333, 0.5124874227587832, 0.8331943286071727), (0.43899186511834465, 0.8470833333333333, 0.513716471702283, 0.8298582151793161), (0.4386356017986933, 0.8466666666666667, 0.5141545914927556, 0.8281901584653878), (0.4381559082865715, 0.845625, 0.5134110821968719, 0.8273561301084237), (0.43768669545650485, 0.84625, 0.5152170350891635, 0.8256880733944955), (0.43707618991533914, 0.8470833333333333, 0.5141405253434201, 0.8290241868223519), (0.43670131494601566, 0.8441666666666666, 0.5142450705978451, 0.8306922435362802), (0.4362223540743192, 0.8491666666666666, 0.5139237123792424, 0.8298582151793161), (0.4357333681980769, 0.8460416666666667, 0.5147619876789987, 0.8298582151793161), (0.43533587257067363, 0.8479166666666667, 0.5138350265338284, 0.8306922435362802), (0.4345952112476031, 0.8472916666666667, 0.5128843199222459, 0.8306922435362802), (0.43442229787508646, 0.8472916666666667, 0.5126253373181452, 0.8273561301084237), (0.43411187117298444, 0.8495833333333334, 0.5129769097476129, 0.8315262718932444), (0.4335293279091517, 0.8477083333333333, 0.5124340375778176, 0.8306922435362802), (0.433121297955513, 0.8491666666666666, 0.5135537046656398, 0.8290241868223519), (0.4329302703340848, 0.8466666666666667, 0.513255930871542, 0.8265221017514596), (0.43181153178215026, 0.8489583333333334, 0.5169460610412776, 0.8248540450375312), (0.4319839918613434, 0.8470833333333333, 0.51349452528186, 0.8281901584653878), (0.43144616613785425, 0.849375, 0.512303670959934, 0.8306922435362802), (0.43117228627204895, 0.8508333333333333, 0.5130833875670644, 0.8298582151793161), (0.4306680823365847, 0.849375, 0.5136200205001163, 0.8290241868223519), (0.43022252440452574, 0.8485416666666666, 0.5141041751102768, 0.8281901584653878), (0.42971697509288787, 0.8504166666666667, 0.5118292207465359, 0.8298582151793161), (0.42915386776129405, 0.851875, 0.5123504589158759, 0.8281901584653878), (0.4287889627615611, 0.851875, 0.5157337035607854, 0.8273561301084237), (0.4282587531208992, 0.85, 0.5118294939634898, 0.8315262718932444), (0.4284209804733594, 0.8466666666666667, 0.5117099962699801, 0.8306922435362802)]\n",
      "0.8158\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 11\n",
    "epochs = 300\n",
    "init = lambda x: nn.init.xavier_uniform_(\n",
    "    tensor=x) if len(x.shape) > 1 else None\n",
    "active = nn.ReLU\n",
    "optimize = optim.SGD\n",
    "schedule = None\n",
    "weight_decay = 0.0\n",
    "learning_goal = 100.\n",
    "model = TwoLayerNetwork(input_size, hidden_size,\n",
    "                        output_size, init, active)\n",
    "EB_baseline = test(model, device, testloader)\n",
    "EB_history = train(model, optimize, device, epochs, learning_rate,\n",
    "                   trainloader, valloader, criterion, schedule, weight_decay, learning_goal)\n",
    "EB_result = test(model, device, testloader)\n",
    "print(EB_baseline, EB_history, EB_result, sep=\"\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epoch bound and learning goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0657\n",
      "[(2.3361593596140544, 0.08416666666666667, 2.251970630372137, 0.1292743953294412), (2.2178289111455283, 0.175, 2.177703283546963, 0.20683903252710592), (2.1403216234842937, 0.24, 2.0896413039922517, 0.2810675562969141), (2.038500696818034, 0.3125, 1.9596197623029363, 0.37364470391993326), (1.8871492306391398, 0.408125, 1.7951224674474606, 0.451209341117598), (1.733130259513855, 0.46791666666666665, 1.64451773908359, 0.4987489574645538), (1.5931685694058737, 0.5122916666666667, 1.5111697553295806, 0.5254378648874062), (1.472662479877472, 0.5489583333333333, 1.4017814323243944, 0.5763135946622185), (1.3725199794769287, 0.5964583333333333, 1.310238278041391, 0.6146788990825688), (1.2900520916779836, 0.6166666666666667, 1.23744816994846, 0.6255212677231026), (1.221700081427892, 0.6347916666666666, 1.1752855070836352, 0.6321934945788157), (1.164270633459091, 0.6466666666666666, 1.1230694287215004, 0.6413678065054211), (1.115423686504364, 0.6583333333333333, 1.0800473437496183, 0.6563803169307757), (1.0735590294996897, 0.66625, 1.040979115837708, 0.6630525437864887), (1.0374992215633392, 0.6777083333333334, 1.0079783282248154, 0.6697247706422018), (1.0058918865521749, 0.683125, 0.9782407883309244, 0.6847372810675563), (0.9781759977340698, 0.6885416666666667, 0.9524303607089605, 0.6872393661384487), (0.953271009127299, 0.6939583333333333, 0.9288796077080823, 0.6964136780650542), (0.9314183994134267, 0.699375, 0.9093812615101888, 0.7022518765638032), (0.9113941419124604, 0.7058333333333333, 0.8906777273922587, 0.7097581317764804), (0.8930266873041789, 0.7089583333333334, 0.8747356030919137, 0.7089241034195163), (0.8765944945812225, 0.7104166666666667, 0.8585572286483345, 0.7114261884904087), (0.8613583779335022, 0.7122916666666667, 0.8428037189960083, 0.7080900750625522), (0.8472177998224895, 0.714375, 0.8292032038300509, 0.7147623019182652), (0.8344054003556569, 0.7202083333333333, 0.8180239847344692, 0.7197664720600501), (0.822486054499944, 0.7227083333333333, 0.8048175935749217, 0.7231025854879066), (0.8113007261355718, 0.7220833333333333, 0.7938999701182577, 0.725604670558799), (0.8007465934753418, 0.7258333333333333, 0.7850558297946316, 0.7239366138448707), (0.7910149278243382, 0.7279166666666667, 0.7745215482668045, 0.7247706422018348), (0.7817009510596593, 0.7285416666666666, 0.766011507636413, 0.7289407839866555), (0.7726395533482233, 0.7289583333333334, 0.7588713257585197, 0.7297748123436196), (0.7644943052530289, 0.7285416666666666, 0.7491515257937198, 0.7347789824854045), (0.7564121180772782, 0.7335416666666666, 0.7418105640045497, 0.7347789824854045), (0.7492472056547801, 0.73375, 0.733667442110601, 0.7381150959132611), (0.7418824448188146, 0.7341666666666666, 0.7280489128266303, 0.7356130108423686), (0.7351462751626968, 0.736875, 0.7221469335301505, 0.7389491242702252), (0.7291706703106562, 0.7358333333333333, 0.7165661100003001, 0.7406171809841534), (0.7230883699655533, 0.7410416666666667, 0.7081588669256731, 0.7464553794829024), (0.7174418493111928, 0.73875, 0.7044117723135673, 0.7422852376980817), (0.711940008799235, 0.741875, 0.6993104983509532, 0.7464553794829024), (0.7068316570917765, 0.7427083333333333, 0.694687137214018, 0.7472894078398665), (0.7017065860827764, 0.74375, 0.6901524466112119, 0.7464553794829024), (0.6969830558697383, 0.7475, 0.6849476869549723, 0.7539616346955796), (0.6922452129920323, 0.746875, 0.6804439077484697, 0.7531276063386155), (0.6878279127677281, 0.7470833333333333, 0.6754826140761674, 0.755629691409508), (0.6835295548041661, 0.7489583333333333, 0.6732870391550613, 0.7614678899082569), (0.6798015493154526, 0.7491666666666666, 0.6694833784425527, 0.762301918265221), (0.6757421811421712, 0.7522916666666667, 0.6654762181766437, 0.7589658048373644), (0.6717328689495723, 0.754375, 0.6615821904098123, 0.7664720600500416), (0.6679106769959132, 0.7545833333333334, 0.6587750881388349, 0.7614678899082569), (0.6642490762472153, 0.7541666666666667, 0.6566899982564542, 0.7673060884070059), (0.66110877374808, 0.7533333333333333, 0.6520603021748171, 0.7631359466221852), (0.6573788770039877, 0.755625, 0.648646652449957, 0.7639699749791493), (0.654243411620458, 0.7602083333333334, 0.6453316254949848, 0.7673060884070059), (0.6509678381681442, 0.7566666666666667, 0.6416020986932431, 0.773978315262719), (0.6477093201875687, 0.758125, 0.6400744686830631, 0.7706422018348624), (0.6446424104770024, 0.7602083333333334, 0.6359007173125399, 0.7698081734778982), (0.6419573160012563, 0.7608333333333334, 0.6336441579910991, 0.7698081734778982), (0.6389151626825332, 0.7620833333333333, 0.6327728894077012, 0.7731442869057548), (0.636206946571668, 0.763125, 0.6307489041291842, 0.773978315262719), (0.6332360293467839, 0.764375, 0.6281479262033037, 0.7706422018348624), (0.6309120573600133, 0.7633333333333333, 0.6244286587578342, 0.7764804003336113), (0.6281576917568843, 0.7664583333333334, 0.6249427502606688, 0.7731442869057548), (0.6254685306549073, 0.7658333333333334, 0.6191004932274711, 0.780650542118432), (0.6230930296579996, 0.7660416666666666, 0.6169544190441797, 0.7773144286905754), (0.6205274921655655, 0.7691666666666667, 0.6174747996075736, 0.7823185988323603), (0.6179003145297368, 0.7710416666666666, 0.6157074485151245, 0.7839866555462885), (0.61584921002388, 0.77, 0.6118060390187662, 0.7814845704753962), (0.6134324618180593, 0.770625, 0.6099619389673986, 0.7831526271893244), (0.6113024230798085, 0.773125, 0.6082109817869967, 0.7856547122602169), (0.6090722201267879, 0.7714583333333334, 0.6066142538868456, 0.7856547122602169), (0.606905525525411, 0.7739583333333333, 0.6054962138218916, 0.7856547122602169), (0.604694273074468, 0.775, 0.6030443336587831, 0.7881567973311092), (0.6025749075412751, 0.7772916666666667, 0.6017037385498314, 0.7898248540450375), (0.6006737786531449, 0.7758333333333334, 0.6016682596182803, 0.7839866555462885), (0.5986564157406489, 0.7772916666666667, 0.5985209919990749, 0.7881567973311092), (0.5963739422957103, 0.7772916666666667, 0.5951286845648657, 0.7898248540450375), (0.5943955075740814, 0.7791666666666667, 0.5941440338040511, 0.79232693911593), (0.5925440764427186, 0.7802083333333333, 0.5931357988424357, 0.7906588824020017), (0.5906244949499766, 0.78, 0.5907791678462057, 0.7898248540450375), (0.5887956857681275, 0.7825, 0.5903136448824375, 0.7881567973311092), (0.5868721240758896, 0.7833333333333333, 0.5890292515846169, 0.7914929107589658), (0.5852672111988068, 0.7845833333333333, 0.5856924247403658, 0.7906588824020017), (0.5835014090935389, 0.7820833333333334, 0.5866250377132854, 0.7898248540450375), (0.5814748444159825, 0.78625, 0.5847679181234154, 0.7931609674728941), (0.5798214123646418, 0.7847916666666667, 0.5855577074059652, 0.7906588824020017), (0.5782260823249817, 0.7860416666666666, 0.5834908827928029, 0.7956630525437864), (0.576538762251536, 0.7877083333333333, 0.5809548649815741, 0.7956630525437864), (0.5751700625816981, 0.7870833333333334, 0.5797187842310618, 0.7948290241868223), (0.5732836600144704, 0.788125, 0.5784459094189921, 0.7956630525437864), (0.5717106811205546, 0.7883333333333333, 0.5760981500198485, 0.7973311092577148), (0.569628332455953, 0.7891666666666667, 0.5783701366539097, 0.8006672226855713), (0.568772197564443, 0.7910416666666666, 0.5750994705676635, 0.7956630525437864), (0.5666613219181696, 0.790625, 0.5760370904550243, 0.7948290241868223), (0.5653375546137492, 0.7922916666666666, 0.5755738754188945, 0.7939949958298582), (0.5642286023497581, 0.791875, 0.5719336394074561, 0.8015012510425354), (0.56254401187102, 0.7935416666666667, 0.5704987226574495, 0.798999165971643), (0.5608920574188232, 0.7925, 0.5709588597773313, 0.7964970809007507), (0.5593794755140941, 0.795, 0.5672695484630658, 0.8006672226855713), (0.5581388090054195, 0.7939583333333333, 0.5678981443163352, 0.7998331943286072), (0.5565179489056269, 0.7945833333333333, 0.5667060874122893, 0.7981651376146789), (0.5552575006087621, 0.7970833333333334, 0.5654152687437043, 0.8040033361134279), (0.5541579914093018, 0.798125, 0.564021435998896, 0.8040033361134279), (0.5523506883780162, 0.7966666666666666, 0.5634451875396328, 0.8065054211843202), (0.5513053812583287, 0.7979166666666667, 0.5633517215906132, 0.8040033361134279), (0.5499795269966126, 0.7966666666666666, 0.5610202132810445, 0.804837364470392), (0.5484591188033422, 0.7979166666666667, 0.5610241594167428, 0.8056713928273561), (0.5472368077437083, 0.8025, 0.5593322737700149, 0.8056713928273561), (0.5459583252668381, 0.7983333333333333, 0.5593768663959169, 0.804837364470392), (0.5442989414930344, 0.80125, 0.556593113287575, 0.8073394495412844), (0.5430910166104634, 0.8022916666666666, 0.5555910588901574, 0.8056713928273561), (0.5420821672677993, 0.8020833333333334, 0.556174725567529, 0.8056713928273561), (0.540916966398557, 0.803125, 0.5555360581697872, 0.8040033361134279), (0.5395313133796056, 0.8035416666666667, 0.5540234453982368, 0.8081734778982486), (0.5377903972069422, 0.80375, 0.5557563207465673, 0.8073394495412844), (0.5372887939214707, 0.8027083333333334, 0.5527304663968344, 0.8065054211843202), (0.5359909065564473, 0.8058333333333333, 0.5514838891888381, 0.8081734778982486), (0.5348138808210691, 0.8058333333333333, 0.5517063014103633, 0.8056713928273561), (0.5332845956087112, 0.8058333333333333, 0.548987141492667, 0.8090075062552127), (0.5324139235417048, 0.8060416666666667, 0.549115130238179, 0.810675562969141), (0.531206089357535, 0.806875, 0.5502047204195807, 0.8115095913261051), (0.5300101165970167, 0.8064583333333334, 0.5477050636787033, 0.8115095913261051), (0.5289262706041336, 0.809375, 0.5470254686198899, 0.8098415346121768), (0.5276127970218658, 0.8089583333333333, 0.5468113561338738, 0.810675562969141), (0.5264488950371742, 0.8075, 0.546616389912501, 0.8115095913261051), (0.5256185877323151, 0.8083333333333333, 0.5447818904344592, 0.810675562969141), (0.5244019851088524, 0.810625, 0.5439754484393777, 0.8148457047539617), (0.5235783045490583, 0.8097916666666667, 0.5432783254889074, 0.8098415346121768), (0.5222529836495717, 0.81125, 0.5435154125206265, 0.8123436196830692), (0.5212572494149208, 0.8108333333333333, 0.5428249204128955, 0.810675562969141), (0.5198383529980978, 0.8085416666666667, 0.5435002079797447, 0.8115095913261051), (0.518988988995552, 0.8114583333333333, 0.5435301023189777, 0.8131776480400333), (0.5182571377356847, 0.811875, 0.5401337837755332, 0.8165137614678899)]\n",
      "0.7924\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 11\n",
    "epochs = 300\n",
    "def init(x):\n",
    "    return nn.init.xavier_uniform_(tensor=x) if len(x.shape) > 1 else None\n",
    "active = nn.ReLU\n",
    "optimize = optim.SGD\n",
    "schedule = None\n",
    "weight_decay = 0.0\n",
    "learning_goal = EB_result #\n",
    "model = TwoLayerNetwork(input_size, hidden_size,\n",
    "                        output_size, init, active)\n",
    "LG_baseline = test(model, device, testloader)\n",
    "LG_history = train(model, optimize, device, epochs, learning_rate,\n",
    "                trainloader, valloader, criterion, schedule, weight_decay, learning_goal)\n",
    "LG_result = test(model, device, testloader)\n",
    "print(LG_baseline, LG_history, LG_result, sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0725737be4be03859ccf648c604bdce2d511d4addba95219b9055f0ea318ae44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

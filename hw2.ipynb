{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1DSSHN_lRd0A_tPBwYBi6zlOd_9N1DBJ3#scrollTo=dpz7yKFTYXPZ)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW Requirement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â€¢ Implement the code for the 2-layer neural networks in CS231n \n",
    "2021 version with PyTorch (or TensorFlow). \n",
    "\n",
    "â€¢ Once you have the code (regardless of which framework you \n",
    "choose above), you will apply your own data.  The training and test \n",
    "dataset is 80%:20%.\n",
    "\n",
    "â€¢ You need to run the code with the following hyperparameter \n",
    "settings:\n",
    "\n",
    "âœ“ Activation function: tanh, ReLU\n",
    "\n",
    "âœ“ Data preprocessing\n",
    "\n",
    "âœ“ Initial weights: small random number, Xavier or Kaiming/MSRA \n",
    "Initialization\n",
    "\n",
    "âœ“ Loss function: without or with the regularization term \n",
    "(L2), Î» = \n",
    "0.001 or 0.0001\n",
    "$$ E(w) = \\frac{1}{N}\\sum^{N}_{c=1}[ð‘“(X^c, w) âˆ’y^c]^2 \n",
    " + \\lambda[\\sum^{p}_{i=0}(w^{o}_{i})^2\n",
    " + \\sum_{i=1}^{p}\\sum_{j=0}^{m}(w_{ij}^H)^2]\n",
    "$$\n",
    "âœ“ Optimizer: gradient descent, Momentum, Adam\n",
    "\n",
    "âœ“ Learning epochs: 100, 200, 300\n",
    "\n",
    "âœ“ Amount of hidden nodes: 5, 8, 11\n",
    "\n",
    "âœ“ Learning rate decay schedule: none and cosine\n",
    "\n",
    "âœ“ Ensembles: top 3 models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VS_Workplace\\NN\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim, Generator\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import sys\n",
    "import pandas as pd\n",
    "import cProfile\n",
    "import pstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Callable, Type\n",
    "from operator import mul\n",
    "\n",
    "def product(nums: Iterable[Type], func: Callable[[Type, Type], Type] = mul):\n",
    "    def _product(nums):\n",
    "        nonlocal func\n",
    "        if len(nums) == 1:\n",
    "            return nums[0]\n",
    "        return func(nums[-1], _product(nums[:-1]))\n",
    "    try:\n",
    "        return _product(nums)\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVES = {\n",
    "    \"relu\": nn.ReLU,\n",
    "    \"tanh\": nn.Tanh\n",
    "}\n",
    "INIT_FUNCS = {\n",
    "    \"small_random\": lambda x: nn.init.normal_(tensor=x, mean=0, std=0.01),\n",
    "    \"xavier\": lambda x: nn.init.xavier_uniform_(tensor=x) if len(x.shape) > 1 else None,\n",
    "    \"kaiming\": lambda x: nn.init.kaiming_uniform_(tensor=x, nonlinearity='relu') if len(x.shape) > 1 else None\n",
    "}\n",
    "OPTIM_FUNCS = {\n",
    "    \"sgd\": optim.SGD,\n",
    "    \"momentum\": lambda param, lr, weight_decay: optim.SGD(params=param, lr=lr, momentum=0.9, weight_decay=weight_decay),\n",
    "    \"adam\": optim.Adam\n",
    "}\n",
    "SCHEDULERS = {\n",
    "    \"None\": None,\n",
    "    \"cos\": lambda opt: torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=opt, T_max=200)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "class TwoLayerNetwork(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_classes: int, init_method:Callable, active_func:nn.modules.module.Module) -> None:\n",
    "        super(TwoLayerNetwork, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size= hidden_size\n",
    "        ## first layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        ## activation\n",
    "        self.active_func = active_func()\n",
    "        ## initialize\n",
    "        for param in self.parameters():\n",
    "            init_method(param)\n",
    "        ## second layer\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.active_func(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: TwoLayerNetwork, opt: nn.Module, device: str, epochs: int, learning_rate: float, trainloader: DataLoader, valloader: DataLoader, criterion: nn.modules.loss._Loss, sched: optim.lr_scheduler._LRScheduler, weight_decay:float):\n",
    "    if epochs < 1:\n",
    "        raise ValueError(\"Invalid epoch!!\")\n",
    "    else:\n",
    "        epochs = int(epochs)\n",
    "    model.to(device)\n",
    "    optimizer = opt(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = sched(optimizer) if sched else None\n",
    "    history = []\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        model.train()\n",
    "        for X, y in trainloader:\n",
    "            X = X.view(-1, model.input_size).to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_correct += (predicted == y).sum().item()\n",
    "        train_loss /= len(trainloader.dataset)\n",
    "        train_accuracy = train_correct / len(trainloader.dataset)\n",
    "\n",
    "        # Validate the model\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, y in valloader:\n",
    "                X = X.view(-1, model.input_size).to(device)\n",
    "                y = y.to(device)\n",
    "                outputs = model(X)\n",
    "                loss = criterion(outputs, y)\n",
    "                val_loss += loss.item() * X.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_correct += (predicted == y).sum().item()\n",
    "            val_loss /= len(valloader.dataset)\n",
    "            val_accuracy = val_correct / len(valloader.dataset)\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        # Print epoch statistics\n",
    "        history.append((train_loss, train_accuracy, val_loss, val_accuracy))\n",
    "        # sys.stdout.write('Epoch [{}/{}], Train Loss: {:.4f}, Train Accuracy: {:.2f}%, Val Loss: {:.4f}, Val Accuracy: {:.2f}%\\n'\n",
    "        #       .format(epoch+1, epochs, train_loss, train_accuracy, val_loss, val_accuracy))\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model:nn.Module, device:str, testloader:DataLoader):\n",
    "    val_correct = 0\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in testloader:\n",
    "            X = X.view(-1, model.input_size).to(device)\n",
    "            y = y.to(device)\n",
    "            outputs = model(X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_correct += (predicted == y).sum().item()\n",
    "        val_accuracy = val_correct / len(testloader.dataset)\n",
    "    return val_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pytorch dataset\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def getPytorchData(train: float = 0.8, remain: float = 0.1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        train: train_amount / total_amount or 1 - valid_amount / total_amount\n",
    "        remain: reduce data amount to save time\n",
    "    \"\"\"\n",
    "    # preprocess: flatten, normalize, drop 90%, split\n",
    "    transform = transforms.transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    if 0 >= train or train >= 1:\n",
    "        raise ValueError()\n",
    "    if 0 > remain or remain > 1:\n",
    "        raise ValueError()\n",
    "    # Split the training set into training and validation sets\n",
    "    trainset = datasets.FashionMNIST(\n",
    "        root=\"./data/\", train=True, download=False, transform=transform)\n",
    "    train_count = int(train * remain * len(trainset))\n",
    "    valid_count = int((1-train) * remain * len(trainset))\n",
    "    if train_count * valid_count == 0:\n",
    "        raise ValueError()\n",
    "    datum_size = product(trainset[0][0].size())\n",
    "    class_amount = len(trainset.classes)\n",
    "    testset = datasets.FashionMNIST(\n",
    "        root=\"./data/\", train=False, download=False, transform=transform)\n",
    "    print(train_count, valid_count, len(testset))\n",
    "    trainset, valset, _ = random_split(\n",
    "        trainset, (train_count, valid_count, len(trainset)-train_count-valid_count), Generator().manual_seed(42))\n",
    "    # Create dataloaders to load the data in batches\n",
    "    trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "    valloader = DataLoader(valset, batch_size=32, shuffle=True)\n",
    "    testloader = DataLoader(testset, batch_size=32, shuffle=True)\n",
    "    return trainloader, valloader, testloader, datum_size, class_amount\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### customized pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "class HotelReservationDataset(Dataset):\n",
    "    \"\"\"Hotel Reservation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        # 19\n",
    "        reservations = pd.read_csv(csv_path)\n",
    "        # 5\n",
    "        for col in map(lambda x: x[0], filter(lambda x:x[1]==\"O\", reservations.dtypes.items())):\n",
    "            d = dict((j, i) for i, j in enumerate(reservations[col].value_counts().index))\n",
    "            setattr(self, f\"labels_of_{col}\", d.keys())\n",
    "            reservations[col]=reservations[col].map(d.__getitem__)\n",
    "        # 17(drop id)\n",
    "        self.feature = torch.from_numpy(reservations.iloc[:, 1:-1].to_numpy(dtype=np.float32))\n",
    "        # two status\n",
    "        self.booking_status = torch.reshape(torch.tensor(reservations.iloc[:, -1:].to_numpy()), shape=(len(self.feature),))\n",
    "        self.classes = list(getattr(self, f\"labels_of_{reservations.columns[-1]}\"))\n",
    "    def __len__(self):\n",
    "        return len(self.booking_status)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self.feature[idx], self.booking_status[idx]\n",
    "\n",
    "# kaggle: ahsan81/hotel-reservations-classification-dataset\n",
    "def getCustomizedData():\n",
    "    # preprocess\n",
    "    dataset = HotelReservationDataset(\n",
    "        csv_path=r\"D:\\dataset\\archive\\Hotel Reservations.csv\")\n",
    "    class_amount = len(dataset.classes)\n",
    "    # train test split\n",
    "    train_count = int(0.7 * len(dataset))\n",
    "    valid_count = int(0.2 * len(dataset))\n",
    "    test_count = len(dataset) - train_count - valid_count\n",
    "    print(train_count, valid_count, test_count)\n",
    "    trainset, valset, testset = random_split(\n",
    "        dataset, (train_count, valid_count, test_count), Generator().manual_seed(42))\n",
    "    datum_size = product(trainset[0][0].size())\n",
    "    # set loaders\n",
    "    trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "    valloader = DataLoader(valset, batch_size=32, shuffle=True)\n",
    "    testloader = DataLoader(testset, batch_size=32, shuffle=True)\n",
    "    return trainloader, valloader, testloader, datum_size, class_amount\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data(zipped csv) from kaggle with username and apikey\n",
    "import os\n",
    "import json\n",
    "with open(\"kaggle.json\", \"r\") as j:\n",
    "    for (k, v) in json.load(j).items():\n",
    "        os.environ[k] = v\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "# https://www.kaggle.com/datasets/uciml/iris/download?datasetVersionNumber=2\n",
    "# owner/datasetname\n",
    "api.dataset_download_files('uciml/iris', path=\"./data/\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_schedule():\n",
    "    counter = 0\n",
    "    # processor\n",
    "    device = \"cuda\" if torch.cuda.is_available(\n",
    "    ) else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    # hyper parameters\n",
    "    trainloader, valloader, testloader, input_size, output_size = getPytorchData()\n",
    "    learning_rate = 0.001\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # âœ“ Amount of hidden nodes: 5, 8, 11\n",
    "    for hidden_size in (5, 8, 11):\n",
    "        # âœ“ Learning epochs: 100, 200, 300\n",
    "        for epochs in (100, 200, 300):\n",
    "            # Create model, optimizer, scheduler\n",
    "            for (init, method) in INIT_FUNCS.items():\n",
    "                for (active, func) in ACTIVES.items():\n",
    "                    # âœ“ Activation function: tanh, ReLU\n",
    "                    # âœ“ Initial weights: small random number, Xavier or Kaiming/MSRA Initialization\n",
    "                    model = TwoLayerNetwork(input_size, hidden_size, output_size,\n",
    "                                            init_method=method, active_func=func).to(device)\n",
    "                    # âœ“ Optimizer: gradient descent, Momentum, Adam\n",
    "                    for (optimize, optm) in OPTIM_FUNCS.items():\n",
    "                        # âœ“ Learning rate decay schedule: none and cosine\n",
    "                        for (schedule, schd) in SCHEDULERS.items():\n",
    "                            # âœ“ Loss function: without or with L2, Î» = 0.001 or 0.0001\n",
    "                            for weight_decay in (0.0, 0.001, 0.0001):\n",
    "                                with open(f\"./data/{counter}.txt\", \"w\") as f:\n",
    "                                    sys.stdout = f\n",
    "                                    f.write(f\"{counter}: {hidden_size}, {epochs}, {init}, {active}, {optimize}, {schedule}, {weight_decay}\\n\")\n",
    "                                    counter+=1\n",
    "                                    test(model=model, device=device, testloader=testloader)\n",
    "                                    break\n",
    "                                    train(model=model, opt=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "                                        trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "                                    test(model=model, device=device, testloader=testloader)\n",
    "                            break\n",
    "                        break\n",
    "                    break\n",
    "                break\n",
    "            break\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schedule\n",
    "def createSchedule(path: str):\n",
    "    hidden_size = (5, 8, 11)\n",
    "    epochs = (100, 200, 300)\n",
    "    init = tuple(INIT_FUNCS.keys())\n",
    "    active = tuple(ACTIVES.keys())\n",
    "    optimize = tuple(OPTIM_FUNCS.keys())\n",
    "    schd = tuple(SCHEDULERS.keys())\n",
    "    weight_decay = (0.0, 0.001, 0.0001)\n",
    "    total_count = (len(hidden_size) + len(epochs) + len(init) +\n",
    "                   len(active) + len(optimize) + len(schd) + len(weight_decay))\n",
    "    column_names = [\"hidden_size\", \"epochs\", \"schd\",\n",
    "                    \"weight_decay\", \"init\", \"active\", \"optimize\"]\n",
    "    column = [hidden_size, epochs, schd, weight_decay, init, active, optimize]\n",
    "    pd.DataFrame(dict(zip(column_names,\n",
    "                          map(lambda c: c + (c[0],)*(total_count-len(c)), column)))\n",
    "                 ).to_csv(path_or_buf=path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "createSchedule(r\"./data/schedule.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _training_schedule():\n",
    "    def _training(counter, hidden_size, epochs, weight_decay, init, active, optimize, schd):\n",
    "        nonlocal args\n",
    "        (trainloader, valloader, testloader, input_size,\n",
    "         output_size, learning_rate, criterion, device) = args\n",
    "        model = TwoLayerNetwork(input_size, hidden_size,\n",
    "                                output_size, INIT_FUNCS[init], ACTIVES[active])\n",
    "        with open(f\"./data/{counter}.txt\", \"w\") as f:\n",
    "            sys.stdout = f\n",
    "            sys.stdout.write(\n",
    "                f\"{counter}: {hidden_size}, {epochs}, {init}, {active}, {optimize}, {schd}, {weight_decay}\\n\")\n",
    "            test(model, device, testloader)\n",
    "            train(model, OPTIM_FUNCS[optimize], device, epochs, learning_rate,\n",
    "                  trainloader, valloader, criterion, SCHEDULERS[schd], weight_decay)\n",
    "            test(model, device, testloader)\n",
    "\n",
    "    # processor\n",
    "    device = \"cuda\" if torch.cuda.is_available(\n",
    "    ) else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    # hyper parameters\n",
    "    trainloader, valloader, testloader, input_size, output_size = getPytorchData()\n",
    "    learning_rate = 0.001\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    args = (trainloader, valloader, testloader, input_size,\n",
    "            output_size, learning_rate, criterion, device)\n",
    "    df = pd.read_csv(r\"./data/schedule.csv\")\n",
    "    df[\"counter\"] = df.index\n",
    "    return df, _training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800 1199 10000\n"
     ]
    }
   ],
   "source": [
    "df, _training = _training_schedule()\n",
    "for index, row in df.iterrows():\n",
    "    # cProfile.run(\"_training(**row)\", \"result.out\")\n",
    "    _training(**row)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pstats.Stats(\"result.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p.strip_dirs().sort_stats(\"tottime\").print_stats())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800 1199 10000\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available(\n",
    ") else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "# hyper parameters\n",
    "trainloader, valloader, testloader, input_size, output_size = getPytorchData()\n",
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.04 [(2.2543232107162474, 15.041666666666666, 2.2057615469852223, 18.34862385321101), (2.179066778818766, 18.395833333333332, 2.152984736659708, 16.09674728940784), (2.1329888979593914, 18.6875, 2.1124607562223408, 20.266889074228523), (2.0931432008743287, 21.75, 2.0746849438267216, 22.68557130942452), (2.0544890411694845, 23.895833333333332, 2.0372910644731688, 25.020850708924105), (2.015653196970622, 25.916666666666668, 1.9996665553116022, 25.938281901584652), (1.9763879760106404, 26.604166666666668, 1.9614586758553931, 28.10675562969141), (1.9368771759668986, 28.416666666666668, 1.9232837591099678, 29.608006672226857), (1.897013602256775, 29.333333333333332, 1.8852086180542984, 30.275229357798164), (1.8569814610481261, 29.729166666666668, 1.846724114585062, 32.11009174311926), (1.8174788411458334, 31.375, 1.8080830415951599, 34.528773978315265), (1.7780001473426819, 34.354166666666664, 1.7699393387930507, 36.44703919933278), (1.7397701334953308, 36.291666666666664, 1.7330190956443423, 36.44703919933278), (1.702611093521118, 36.604166666666664, 1.6972383953711707, 36.94745621351126), (1.6672109182675678, 37.041666666666664, 1.6630585094210106, 36.86405337781485), (1.6336133122444152, 36.625, 1.6305137583372293, 37.1976647206005), (1.6016934243837992, 37.0, 1.5995221739515253, 37.86488740617181), (1.5717506702740986, 37.5, 1.5696584077752362, 37.28106755629691), (1.5429661138852437, 37.583333333333336, 1.5409908669505943, 38.03169307756464), (1.5157004189491272, 37.854166666666664, 1.5129061550573073, 38.11509591326105), (1.4893784848848979, 38.229166666666664, 1.4860052010533809, 38.03169307756464), (1.4638211425145466, 38.125, 1.4599265350313162, 38.61551292743953), (1.4387841693560282, 39.1875, 1.4332886439149235, 39.616346955796494), (1.4142915654182433, 41.166666666666664, 1.4080031057314837, 43.86989157631359), (1.3903131127357482, 45.625, 1.3822790201352575, 48.54045037531276), (1.3670345910390218, 48.020833333333336, 1.358150600094513, 50.29190992493745), (1.3442829426129659, 50.270833333333336, 1.3342426297463805, 51.95996663886572), (1.322221585114797, 51.854166666666664, 1.3116003544356447, 53.211009174311926), (1.3009308890501659, 53.1875, 1.2895682115570717, 56.04670558798999), (1.280623613993327, 55.520833333333336, 1.2682681748825277, 56.38031693077565), (1.260773903131485, 57.145833333333336, 1.2467151795753943, 58.29858215179316), (1.241705712477366, 58.020833333333336, 1.2270968321266524, 59.966638865721436), (1.2226936074097952, 59.416666666666664, 1.2070963195803963, 60.88407005838199), (1.2043399302164715, 60.375, 1.1883855908984835, 61.05087572977481), (1.1859109993775685, 61.0, 1.1693685909327713, 61.80150125104254), (1.1680613338947297, 61.479166666666664, 1.1495496215573742, 62.55212677231026), (1.1502885635693867, 61.854166666666664, 1.1309450518796602, 62.71893244370309), (1.1327020665009817, 62.0625, 1.1134137456073077, 62.96914095079233), (1.1153953516483306, 62.1875, 1.0948240319523244, 63.30275229357798), (1.0979402156670888, 62.583333333333336, 1.075875223776616, 63.46955796497081), (1.0811127694447835, 62.5625, 1.058097272241384, 64.55379482902418), (1.0641473762194316, 62.833333333333336, 1.0404512272763988, 64.6371976647206), (1.0470961765448252, 63.208333333333336, 1.0241403962494036, 63.88657214345288), (1.0304639593760172, 63.333333333333336, 1.0054683912983529, 63.96997497914929), (1.0134401651223501, 63.479166666666664, 0.9879707678145822, 64.13678065054212), (0.997178084452947, 63.770833333333336, 0.9713386018838954, 64.22018348623853), (0.9816342262427012, 63.833333333333336, 0.9551189531377199, 64.6371976647206), (0.9667074843247732, 64.02083333333333, 0.9404372369079812, 64.97080900750626), (0.9528820923964183, 64.5625, 0.9253080805904971, 65.55462885738115), (0.939102797905604, 64.89583333333333, 0.9118584359159462, 65.8882402001668), (0.9268108280499776, 65.79166666666667, 0.8992093609610232, 66.80567139282735), (0.914977590640386, 66.60416666666667, 0.886611915708483, 67.13928273561301), (0.9038111372788747, 67.52083333333333, 0.8763491925942689, 68.05671392827357), (0.8926735945542653, 68.35416666666667, 0.8657054439796021, 68.39032527105921), (0.8826516890525817, 69.47916666666667, 0.8548612980866452, 69.22435362802335), (0.8726184666156769, 70.0625, 0.8444150350708282, 69.64136780650541), (0.8628408622741699, 70.70833333333333, 0.8358442060046637, 70.30859049207673), (0.8531985851128896, 71.0, 0.8265030395497472, 71.47623019182652), (0.8438104546070099, 71.54166666666667, 0.8170858645459033, 72.39366138448707), (0.8350015385945638, 71.72916666666667, 0.809441932049068, 72.4770642201835), (0.8270994567871094, 71.95833333333333, 0.8012364554942102, 73.06088407005838), (0.8190656089782715, 72.22916666666667, 0.7935591131274754, 73.47789824854046), (0.811228875319163, 72.70833333333333, 0.7862850844213821, 73.47789824854046), (0.8042689700921376, 72.45833333333333, 0.7804777556105988, 73.89491242702252), (0.7973811848958333, 72.89583333333333, 0.7732381253465203, 73.89491242702252), (0.7906413014729817, 73.33333333333333, 0.767455941642494, 74.31192660550458), (0.784182738463084, 73.375, 0.7618011196421225, 74.395329441201), (0.7781039077043533, 73.625, 0.7562931060194472, 74.395329441201), (0.7720194548368454, 74.04166666666667, 0.7502658681336595, 75.14595496246872), (0.7662526873747507, 73.95833333333333, 0.7446594559917656, 74.9791492910759), (0.7605976096789042, 74.0625, 0.74016314660439, 74.81234361968306), (0.7547594145933787, 74.04166666666667, 0.7342094726817026, 74.89574645537948), (0.7497996165355046, 74.35416666666667, 0.731165757569002, 74.395329441201), (0.7447452725966771, 74.33333333333333, 0.7263856457858606, 75.31276063386156), (0.7399470496177674, 74.77083333333333, 0.7211859597575178, 75.14595496246872), (0.7353538233041763, 74.8125, 0.7161183236438697, 75.31276063386156), (0.7302034042278925, 74.6875, 0.7118091418705352, 75.5629691409508), (0.7261262333393097, 75.0625, 0.7096962492798049, 75.81317764804004), (0.7217188398043315, 75.35416666666667, 0.7054754172741522, 75.6463719766472), (0.7173839781681697, 75.33333333333333, 0.7020900683963766, 75.81317764804004), (0.7137645544608434, 75.52083333333333, 0.6987758916154119, 76.14678899082568), (0.7098274012406667, 75.58333333333333, 0.6945375004741329, 76.31359466221852), (0.7059073201815287, 75.75, 0.6917707306628827, 76.98081734778982), (0.7020373161633809, 75.85416666666667, 0.6885463313772044, 76.73060884070058), (0.6987333929538727, 76.14583333333333, 0.6855152700223756, 76.31359466221852), (0.6956662889321645, 76.04166666666667, 0.681733825586556, 77.06422018348624), (0.6921618582804998, 76.25, 0.6789501945707975, 76.814011676397), (0.6887845784425736, 76.16666666666667, 0.679250412279611, 77.06422018348624), (0.6860659418503443, 76.4375, 0.6752510569411779, 76.73060884070058), (0.682850577433904, 76.14583333333333, 0.6731496761499394, 76.98081734778982), (0.6796919800837835, 76.33333333333333, 0.669930682319517, 77.14762301918265), (0.6770180515448252, 76.66666666666667, 0.6663918969678918, 77.31442869057548), (0.6742363299926122, 76.72916666666667, 0.6650844544942822, 77.14762301918265), (0.6714702701568603, 76.625, 0.6637861931741585, 77.81484570475396), (0.6691906185944875, 76.6875, 0.6616503619670471, 77.73144286905755), (0.6664596086740494, 76.83333333333333, 0.6586163767782025, 77.56463719766472), (0.6635891395807266, 76.91666666666667, 0.658546363889823, 77.39783152627189), (0.6614885848760604, 77.0, 0.6570619941254872, 77.89824854045038), (0.6590793281793594, 76.89583333333333, 0.654131724945797, 77.98165137614679), (0.656865597764651, 77.04166666666667, 0.6521525227894278, 77.98165137614679)] 76.42\n"
     ]
    }
   ],
   "source": [
    "def _training(row):\n",
    "    model = TwoLayerNetwork(input_size, row[\"hidden_size\"],\n",
    "                            output_size, INIT_FUNCS[row[\"init\"]], ACTIVES[row[\"active\"]])\n",
    "    baseline = test(model, device, testloader)\n",
    "    history = train(model, OPTIM_FUNCS[row[\"optimize\"]], device, row[\"epochs\"], learning_rate,\n",
    "          trainloader, valloader, criterion, SCHEDULERS[row[\"schd\"]], row[\"weight_decay\"])\n",
    "    result = test(model, device, testloader)\n",
    "    return baseline, history, result\n",
    "\n",
    "df = pd.read_csv(r\"./data/schedule.csv\")\n",
    "df[\"counter\"] = df.index\n",
    "for index, row in df.iterrows():\n",
    "    cProfile.run(\"baseline, history, result = _training(row)\", \"result.out\")\n",
    "    print(baseline, history, result)\n",
    "    break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "results = []\n",
    "for i in range(1, 19):\n",
    "    with open(f\"./data/{i}.txt\", \"r\") as f:\n",
    "        l = list(f.readlines())\n",
    "        accuracy = float(l[len(l) - 1][14:-2])\n",
    "        hidden_size, epochs, init_func, active_func, optimizer, lr_scheduler, weight_decay \\\n",
    "            = l[0][:-1].split(\", \")\n",
    "        hidden_size = int(hidden_size)\n",
    "        epochs = int(epochs)\n",
    "        weight_decay = float(weight_decay)\n",
    "        results.append([hidden_size, epochs, init_func, active_func, optimizer, lr_scheduler, weight_decay, accuracy])\n",
    "        continue\n",
    "        history = pd.DataFrame(\n",
    "            map(lambda r: r[:-1].split(\", \"), l[1:len(l)-1]))\n",
    "        history.columns = [\"epoch\", \"train_loss\",\n",
    "                           \"train_acc\", \"val_loss\", \"val_acc\"]\n",
    "        history.epoch = history.epoch.map(\n",
    "            lambda epoch: int(re.search(r\"\\[(.)*?/\", epoch)[0][1:-1]))\n",
    "        history.train_loss = history.train_loss.map(lambda value: float(\n",
    "            re.search(r\":\\s(.)*\", value)[0][2:]))\n",
    "        history.val_loss = history.val_loss.map(\n",
    "            lambda value: float(re.search(r\":\\s(.)*\", value)[0][2:]))\n",
    "        history.train_acc = history.train_acc.map(lambda value: float(\n",
    "            re.search(r\":\\s(.)*\", value)[0][2:-1])*0.01)\n",
    "        history.val_acc = history.val_acc.map(lambda value: float(\n",
    "            re.search(r\":\\s(.)*\", value)[0][2:-1])*0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results.sort(key=lambda x:x[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11, 200, 'small_random', 'relu', 'sgd', 'None', 0.0, 81.89],\n",
       " [11, 300, 'small_random', 'relu', 'sgd', 'None', 0.0, 82.41],\n",
       " [11, 300, 'small_random', 'relu', 'sgd', 'None', 0.0, 82.45]]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0725737be4be03859ccf648c604bdce2d511d4addba95219b9055f0ea318ae44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

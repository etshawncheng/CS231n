{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1DSSHN_lRd0A_tPBwYBi6zlOd_9N1DBJ3#scrollTo=dpz7yKFTYXPZ)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW Requirement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Make the code of tuning_1 module.\\\n",
    "• Make the code of tuning_2 module.\\\n",
    "• Make the code of tuning_3 module.\\\n",
    "• Make the code of tuning_4 module."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim, Generator\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from numpy.random import choice\n",
    "from typing import Iterable, Callable, Type, Optional, Union, Tuple, List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import mul\n",
    "\n",
    "def product(nums: Iterable[Type], func: Callable[[Type, Type], Type] = mul):\n",
    "    \"\"\"\n",
    "    return product of iterable multiplicable\n",
    "    \"\"\"\n",
    "    def _product(nums):\n",
    "        nonlocal func\n",
    "        if len(nums) == 1:\n",
    "            return nums[0]\n",
    "        return func(nums[-1], _product(nums[:-1]))\n",
    "    try:\n",
    "        return _product(nums)\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "\n",
    "class TwoLayerNetwork(nn.Module):\n",
    "    storage: deque[nn.Module]\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_classes: int, init_method: Callable, active_func: Callable[[], nn.modules.module.Module],\n",
    "                 DO: float, if_BN: bool, store_size: int = 1):\n",
    "        super(TwoLayerNetwork, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.if_BN = if_BN\n",
    "        # dropout\n",
    "        self.do = nn.Dropout(DO)\n",
    "        # first layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # batch norm\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        # activation\n",
    "        self.active_func = active_func()\n",
    "        # second layer\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        # initialize\n",
    "        for param in self.parameters():\n",
    "            init_method(param)\n",
    "        self.storage = deque(maxlen=store_size)\n",
    "\n",
    "    def forward(self, x: Iterable[Union[torch.Tensor, float]]) -> torch.Tensor:\n",
    "        out = self.do(x)\n",
    "        out = self.fc1(out)\n",
    "        if self.if_BN:\n",
    "            out = self.bn1(out)\n",
    "        out = self.active_func(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WD_Regularization(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WD_Regularization, self).__init__()\n",
    "\n",
    "\n",
    "class L2_Regularization(WD_Regularization):\n",
    "    def __init__(self, weight_decay: float):\n",
    "        super(L2_Regularization, self).__init__()\n",
    "        if weight_decay <= 0:\n",
    "            raise ValueError(\"param weight_decay can not <=0!!\")\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, model: nn.Module) -> Union[torch.Tensor, float]:\n",
    "        reg = 0\n",
    "        for name, parameter in model.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                reg += torch.sum(parameter**2)\n",
    "        return self.weight_decay * reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model: nn.Module, device: str, valloader: DataLoader[Dataset[torch.Tensor]], criterion: nn.modules.loss._Loss) \\\n",
    "        -> Tuple[float, float]:\n",
    "    \"\"\"return loss, accuracy\"\"\"\n",
    "    # Validate the model\n",
    "    model.to(device)\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in valloader:\n",
    "            X = X.view(-1, model.input_size).to(device)\n",
    "            y = y.to(device)\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            val_loss += loss.item() * X.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_correct += (predicted == y).sum().item()\n",
    "        val_loss /= len(valloader.dataset)\n",
    "        val_accuracy = val_correct / len(valloader.dataset)\n",
    "    return val_loss, val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: TwoLayerNetwork, opt: Callable[..., optim.Optimizer], device: str, epochs: float, learning_rate: float, trainloader: DataLoader[Dataset[torch.Tensor]], valloader: DataLoader[Dataset[torch.Tensor]], criterion: nn.modules.loss._Loss,\n",
    "          sched: Optional[Callable[[optim.Optimizer], optim.lr_scheduler._LRScheduler]], wd_reg: Optional[WD_Regularization], learning_goal: float, min_lr: float, if_lr_adjust: bool, if_BN: bool, drop_out: float) \\\n",
    "        -> List[Tuple[float, float, float, float]]:\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        model\n",
    "        opt\n",
    "        device\n",
    "        epochs\n",
    "        learing_rate\n",
    "        criterion\n",
    "        y: label of data\n",
    "        wd_reg, BN, DO: regularization\n",
    "    Results:\n",
    "        history: train_loss, train_accuracy, val_loss, val_accuracy of each epochs\n",
    "    \"\"\"\n",
    "    def forward_backward(optimizer: optim.Optimizer, criterion: nn.modules.loss._Loss, wd_reg: Optional[WD_Regularization], model: TwoLayerNetwork, y: torch.Tensor,\n",
    "                         BN: Optional[nn.modules.batchnorm._BatchNorm], DO: Optional[nn.modules.dropout._DropoutNd]) \\\n",
    "            -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            optimizer\n",
    "            criterion\n",
    "            model\n",
    "            y: label of data\n",
    "            wd_reg, BN, DO: regularization\n",
    "        Results:\n",
    "            ouputs: f(x)\n",
    "            loss_all: f(x) - y\n",
    "        \"\"\"\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        outputs = outputs if not DO else DO(outputs)\n",
    "        loss_all = criterion(\n",
    "            outputs, y) + wd_reg(model) if wd_reg else criterion(outputs, y)\n",
    "        loss_all.backward()\n",
    "        optimizer.step()\n",
    "        return loss_all, outputs\n",
    "    if epochs < 1:\n",
    "        raise ValueError(\"Invalid epoch!!\")\n",
    "    if drop_out >= 1 or drop_out < 0:\n",
    "        raise ValueError(\"Invalid dropout rate!!\")\n",
    "    # init\n",
    "    epoch = 0\n",
    "    init_lr = learning_rate\n",
    "    origin_if_BN = model.if_BN\n",
    "    model.if_BN = if_BN\n",
    "    pre_loss = float(\"inf\") if if_lr_adjust else None\n",
    "    BN = nn.BatchNorm1d(model.hidden_size).to(device) if if_BN else None\n",
    "    DO = nn.Dropout(drop_out).to(device) if drop_out != 0. else None\n",
    "    model.to(device)\n",
    "    # if not model.storage[-1]\n",
    "    model.storage.append(list(model.parameters()))\n",
    "    optimizer = opt(model.storage[-1], lr=learning_rate)\n",
    "    scheduler = sched(optimizer) if sched else None\n",
    "    history = []\n",
    "    # Train the model\n",
    "    while epoch < epochs:\n",
    "        # Train the model\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        model.train()\n",
    "        for X, y in trainloader:\n",
    "            X = X.view(-1, model.input_size).to(device)\n",
    "            y = y.to(device)\n",
    "            loss_all, outputs = forward_backward(\n",
    "                optimizer, criterion, wd_reg, model, y, BN, DO)\n",
    "            if pre_loss:\n",
    "                while pre_loss <= loss_all.item():\n",
    "                    if learning_rate < min_lr:\n",
    "                        # return history\n",
    "                        learning_rate = init_lr\n",
    "                        optimizer = opt(model.storage[-1], lr=learning_rate)\n",
    "                        loss_all, outputs = forward_backward(\n",
    "                            optimizer, criterion, wd_reg, model, y, BN, DO)\n",
    "                        # raise ValueError(f\"{learning_rate} < {min_lr}\")\n",
    "                        break\n",
    "                    learning_rate *= 0.7\n",
    "                    optimizer = opt(model.storage[-1], lr=learning_rate)\n",
    "                    loss_all, outputs = forward_backward(\n",
    "                        optimizer, criterion, wd_reg, model, y, BN, DO)\n",
    "                learning_rate *= 1.2\n",
    "                pre_loss = loss_all.item()\n",
    "            train_loss += loss_all.item() * X.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_correct += (predicted == y).sum().item()\n",
    "            model.storage.append(list(model.parameters()))\n",
    "        train_loss /= len(trainloader.dataset)\n",
    "        train_accuracy = train_correct / len(trainloader.dataset)\n",
    "        # Validate the model\n",
    "        val_loss, val_accuracy = validate(\n",
    "            model=model, device=device, valloader=valloader, criterion=criterion)\n",
    "        # Log Statics\n",
    "        history.append((train_loss, train_accuracy, val_loss, val_accuracy))\n",
    "        # Stopping criteria\n",
    "        if learning_goal < val_accuracy:\n",
    "            return history\n",
    "        # Update loop\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        epoch += 1\n",
    "    # restore model\n",
    "    model.if_BN = origin_if_BN\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model: nn.Module, device: str, testloader: DataLoader[Dataset[torch.Tensor]]) -> float:\n",
    "    \"\"\"return accuracy\"\"\"\n",
    "    return validate(model=model, device=device, valloader=testloader, criterion=nn.CrossEntropyLoss())[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reduced_model(model: TwoLayerNetwork, node_id: int):\n",
    "    new_model = TwoLayerNetwork(model.input_size, model.hidden_size - 1, product(\n",
    "        model.fc2.bias.size()), lambda _: None, lambda: model.active_func, model.do.p, model.if_BN)\n",
    "    for name, param in model.named_parameters():\n",
    "        layer_name, variable_type = name.split(\".\")\n",
    "        if layer_name in (\"fc1\", \"bn1\"):\n",
    "            # node size of specified hidden layer, node size of input layer\n",
    "            setattr(getattr(new_model, layer_name), variable_type, nn.Parameter(torch.cat(\n",
    "                (param[:node_id], param[node_id + 1:]), 0)))\n",
    "        elif layer_name == \"fc2\" and variable_type == \"weight\":\n",
    "            # node size of output layer, node size of specified hidden layer\n",
    "            setattr(getattr(new_model, layer_name), variable_type, nn.Parameter(torch.cat(\n",
    "                (param[:, :node_id], param[:, node_id + 1:]), 1)))\n",
    "        else:\n",
    "            pass\n",
    "    return new_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tunning1(model: TwoLayerNetwork, device: str, valloader: DataLoader[Dataset[torch.Tensor]], criterion: nn.modules.loss._Loss,\n",
    "            reg_method: Callable, learning_goal: float):\n",
    "    test_index = 0\n",
    "    cur = pre = model\n",
    "    while test_index < pre.hidden_size:\n",
    "        cur = generate_reduced_model(pre, test_index)\n",
    "        loss, acc = validate(cur, device, valloader, criterion)\n",
    "        if acc >= learning_goal:\n",
    "            reg_method(cur)\n",
    "            pre = cur\n",
    "        else:\n",
    "            test_index += 1\n",
    "    return cur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tunning3(model: TwoLayerNetwork, device: str, valloader: DataLoader[Dataset[torch.Tensor]], criterion: nn.modules.loss._Loss,\n",
    "             reg_method: Callable, learning_goal: float, fail_tolarance: int = 3):\n",
    "    cur = pre = model\n",
    "    fail_count = 0\n",
    "    # may choose same node but I don't care because it's hard to implement\n",
    "    while fail_count < fail_tolarance and pre.hidden_size > 1:\n",
    "        test_index = choice(1, pre.hidden_size)[0]\n",
    "        cur = generate_reduced_model(pre, test_index)\n",
    "        _, acc = validate(cur, device, valloader, criterion)\n",
    "        if acc >= learning_goal:\n",
    "            reg_method(cur)\n",
    "            pre = cur\n",
    "        else:\n",
    "            fail_count += 1\n",
    "    return cur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tunning4(model: TwoLayerNetwork, device: str, valloader: DataLoader[Dataset[torch.Tensor]], criterion: nn.modules.loss._Loss,\n",
    "             reg_method: Callable, learning_goal: float):\n",
    "    cur = pre = model\n",
    "    try_count = 0\n",
    "    while pre.hidden_size > 1 and pre.hidden_size > try_count:\n",
    "        test_index = sorted(enumerate(pre.get_parameter(\n",
    "            \"fc2.weight\").abs().sum(0)), key=lambda x: x[1])[0 + try_count][0]\n",
    "        cur = generate_reduced_model(pre, test_index)\n",
    "        _, acc = validate(cur, device, valloader, criterion)\n",
    "        if acc >= learning_goal:\n",
    "            reg_method(cur)\n",
    "            pre = cur\n",
    "            try_count = 0\n",
    "        else:\n",
    "            try_count += 1\n",
    "    return cur\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pytorch dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "def getPytorchData(train: float = 0.8, remain: float = 0.1) \\\n",
    "    -> tuple[DataLoader[Dataset[torch.Tensor]], DataLoader[Dataset[torch.Tensor]], DataLoader[Dataset[torch.Tensor]], int, int]:\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        train: train_amount / total_amount or 1 - valid_amount / total_amount\n",
    "        remain: reduce data amount to save time\n",
    "    Results:\n",
    "        trainloader, valloader, testloader: dataloader\n",
    "        datum_size: size of datum\n",
    "        class_amount: amount of types\n",
    "    \"\"\"\n",
    "    # preprocess: flatten, normalize, drop 90%, split\n",
    "    transform = transforms.transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    if 0 >= train or train >= 1:\n",
    "        raise ValueError()\n",
    "    if 0 > remain or remain > 1:\n",
    "        raise ValueError()\n",
    "    # Split the training set into training and validation sets\n",
    "    trainset = datasets.FashionMNIST(\n",
    "        root=\"./data/\", train=True, download=False, transform=transform)\n",
    "    train_count = int(train * remain * len(trainset))\n",
    "    valid_count = int((1 - train) * remain * len(trainset))\n",
    "    if train_count * valid_count == 0:\n",
    "        raise ValueError()\n",
    "    datum_size = product(trainset[0][0].size())\n",
    "    class_amount = len(trainset.classes)\n",
    "    testset = datasets.FashionMNIST(\n",
    "        root=\"./data/\", train=False, download=False, transform=transform)\n",
    "    print(train_count, valid_count, len(testset))\n",
    "    trainset, valset, _ = random_split(\n",
    "        trainset, (train_count, valid_count, len(trainset) - train_count - valid_count), Generator().manual_seed(42))\n",
    "    # Create dataloaders to load the data in batches\n",
    "    trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "    valloader = DataLoader(valset, batch_size=32, shuffle=True)\n",
    "    testloader = DataLoader(testset, batch_size=32, shuffle=True)\n",
    "    return trainloader, valloader, testloader, datum_size, class_amount\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Tunning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800 1199 10000\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available(\n",
    ") else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "trainloader, valloader, testloader, input_size, output_size = getPytorchData()\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lg = 0.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TwoLayerNetwork(\n",
       "  (do): Dropout(p=0.2, inplace=False)\n",
       "  (fc1): Linear(in_features=784, out_features=11, bias=True)\n",
       "  (bn1): BatchNorm1d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (active_func): ReLU()\n",
       "  (fc2): Linear(in_features=11, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = r\"./data/rg_eb_lg_ua_bn_do\"\n",
    "model = torch.load(model_path)\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tunning 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TwoLayerNetwork(\n",
       "  (do): Dropout(p=0.2, inplace=False)\n",
       "  (fc1): Linear(in_features=784, out_features=10, bias=True)\n",
       "  (bn1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (active_func): ReLU()\n",
       "  (fc2): Linear(in_features=10, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 50\n",
    "optimize = optim.SGD\n",
    "schedule = None\n",
    "learning_goal = 1. #\n",
    "min_lr = learning_rate * 1e-5\n",
    "l2_reg = None\n",
    "r_BN = lambda x: train(x, optimize, device, epochs, learning_rate,\n",
    "                   trainloader, valloader, criterion, schedule, l2_reg, learning_goal, min_lr, False, True, 0.0)\n",
    "model = torch.load(model_path)\n",
    "model = tunning1(model, device, valloader, criterion, r_BN, lg)\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tunning 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TwoLayerNetwork(\n",
       "  (do): Dropout(p=0.2, inplace=False)\n",
       "  (fc1): Linear(in_features=784, out_features=10, bias=True)\n",
       "  (bn1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (active_func): ReLU()\n",
       "  (fc2): Linear(in_features=10, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 50\n",
    "optimize = optim.SGD\n",
    "schedule = None\n",
    "learning_goal = 1.\n",
    "min_lr = learning_rate * 1e-5\n",
    "l2_reg = None\n",
    "r_BN = lambda x: train(x, optimize, device, epochs, learning_rate,\n",
    "                       trainloader, valloader, criterion, schedule, l2_reg, learning_goal, min_lr, False, True, 0.0)\n",
    "epochs = 300\n",
    "optimize = optim.SGD\n",
    "schedule = None\n",
    "learning_goal = lg\n",
    "min_lr = learning_rate * 1e-5\n",
    "l2_reg = L2_Regularization(weight_decay)\n",
    "rG_EB_LG_UA_BN = lambda x: train(x, optimize, device, epochs, learning_rate,\n",
    "                                 trainloader, valloader, criterion, schedule, l2_reg, learning_goal, min_lr, True, True, 0.0)\n",
    "\n",
    "model = torch.load(model_path)\n",
    "model = tunning1(model, device, valloader, criterion,\n",
    "                 lambda x: r_BN(rG_EB_LG_UA_BN(x)), lg)\n",
    "model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tunning 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TwoLayerNetwork(\n",
       "  (do): Dropout(p=0.2, inplace=False)\n",
       "  (fc1): Linear(in_features=784, out_features=10, bias=True)\n",
       "  (bn1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (active_func): ReLU()\n",
       "  (fc2): Linear(in_features=10, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 50\n",
    "optimize = optim.SGD\n",
    "schedule = None\n",
    "learning_goal = 1.\n",
    "min_lr = learning_rate * 1e-5\n",
    "l2_reg = None\n",
    "r_BN = lambda x: train(x, optimize, device, epochs, learning_rate,\n",
    "                       trainloader, valloader, criterion, schedule, l2_reg, learning_goal, min_lr, False, True, 0.0)\n",
    "epochs = 300\n",
    "optimize = optim.SGD\n",
    "schedule = None\n",
    "learning_goal = lg\n",
    "min_lr = learning_rate * 1e-5\n",
    "l2_reg = L2_Regularization(weight_decay)\n",
    "rG_EB_LG_UA_BN = lambda x: train(x, optimize, device, epochs, learning_rate,\n",
    "                                 trainloader, valloader, criterion, schedule, l2_reg, learning_goal, min_lr, True, True, 0.0)\n",
    "\n",
    "model = torch.load(model_path)\n",
    "model = tunning3(model, device, valloader, criterion,\n",
    "                 lambda x: r_BN(rG_EB_LG_UA_BN(x)), lg)\n",
    "model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tunning 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TwoLayerNetwork(\n",
       "  (do): Dropout(p=0.2, inplace=False)\n",
       "  (fc1): Linear(in_features=784, out_features=10, bias=True)\n",
       "  (bn1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (active_func): ReLU()\n",
       "  (fc2): Linear(in_features=10, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 50\n",
    "optimize = optim.SGD\n",
    "schedule = None\n",
    "learning_goal = 1.\n",
    "min_lr = learning_rate * 1e-5\n",
    "l2_reg = None\n",
    "r_BN = lambda x: train(x, optimize, device, epochs, learning_rate,\n",
    "                       trainloader, valloader, criterion, schedule, l2_reg, learning_goal, min_lr, False, True, 0.0)\n",
    "epochs = 300\n",
    "optimize = optim.SGD\n",
    "schedule = None\n",
    "learning_goal = lg\n",
    "min_lr = learning_rate * 1e-5\n",
    "l2_reg = L2_Regularization(weight_decay)\n",
    "rG_EB_LG_UA_BN = lambda x: train(x, optimize, device, epochs, learning_rate,\n",
    "                                 trainloader, valloader, criterion, schedule, l2_reg, learning_goal, min_lr, True, True, 0.0)\n",
    "\n",
    "model = torch.load(model_path)\n",
    "model = tunning4(model, device, valloader, criterion,\n",
    "                 lambda x: r_BN(rG_EB_LG_UA_BN(x)), lg)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0725737be4be03859ccf648c604bdce2d511d4addba95219b9055f0ea318ae44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1DSSHN_lRd0A_tPBwYBi6zlOd_9N1DBJ3#scrollTo=dpz7yKFTYXPZ)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW Requirement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Develop your own WAN algorithm and make the \n",
    "corresponding code.\\\n",
    "• Once you have the code, you will apply the code to learn \n",
    "your dataset to get a better code. Better means the \n",
    "better hyperparameter setting regarding your dataset.\\\n",
    "• The training and test dataset is 80%/20%.\\\n",
    "• The performance comparison benchmark is your best \n",
    "weight-tuning module."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VS_Workplace\\NN\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim, Generator\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from numpy.random import choice\n",
    "from typing import Iterable, Callable, Type, Optional, Union, Tuple, List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import mul\n",
    "\n",
    "\n",
    "def product(nums: Iterable[Type], func: Callable[[Type, Type], Type] = mul) -> Type:\n",
    "    \"\"\"return product of iterable\"\"\"\n",
    "    _it = iter(nums)\n",
    "    v: Type = next(_it)\n",
    "    for _v in _it:\n",
    "        v = func(v, _v)\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "\n",
    "class TwoLayerNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_classes: int, init_method: Callable[[torch.Tensor], torch.Tensor], active_func: Callable[[], nn.modules.module.Module],\n",
    "                 DO: float, if_BN: bool, store_size: int = 1):\n",
    "        super(TwoLayerNetwork, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.if_BN = if_BN\n",
    "        # dropout\n",
    "        self.do = nn.Dropout(DO)\n",
    "        # first layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # batch norm\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        # activation\n",
    "        self.active_func = active_func()\n",
    "        # second layer\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        # initialize\n",
    "        for param in self.parameters():\n",
    "            init_method(param)\n",
    "        self.storage: deque[List[nn.Parameter]] = deque(maxlen=store_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out: torch.Tensor = self.do(x)\n",
    "        out = self.fc1(out)\n",
    "        if self.if_BN:\n",
    "            out = self.bn1(out)\n",
    "        out = self.active_func(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WD_Regularization(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WD_Regularization, self).__init__()\n",
    "\n",
    "\n",
    "class L2_Regularization(WD_Regularization):\n",
    "    def __init__(self, weight_decay: float):\n",
    "        super(L2_Regularization, self).__init__()\n",
    "        if weight_decay <= 0:\n",
    "            raise ValueError(\"param weight_decay can not <=0!!\")\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, model: nn.Module) -> Union[torch.Tensor, float]:\n",
    "        reg = 0\n",
    "        for name, parameter in model.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                reg += torch.sum(parameter**2)\n",
    "        return self.weight_decay * reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model: TwoLayerNetwork, device: str, valset: Dataset[torch.Tensor], criterion: nn.modules.loss._Loss) \\\n",
    "        -> Tuple[float, float]:\n",
    "    \"\"\"return loss, accuracy\"\"\"\n",
    "    # Validate the model\n",
    "    model.to(device)\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in DataLoader(valset, batch_size=32, shuffle=True):\n",
    "            x: torch.Tensor = x.view(-1, model.input_size).to(device)\n",
    "            y: torch.Tensor = y.to(device)\n",
    "            outputs: torch.Tensor = model(x)\n",
    "            loss: torch.Tensor = criterion(outputs, y)\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_correct += (predicted == y).sum().item()\n",
    "        val_loss /= len(valset)\n",
    "        val_accuracy = val_correct / len(valset)\n",
    "    return val_loss, val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: TwoLayerNetwork, opt: Callable[..., optim.Optimizer], device: str, epochs: float, learning_rate: float, trainset: Dataset[torch.Tensor], valset: Dataset[torch.Tensor], criterion: nn.modules.loss._Loss,\n",
    "          sched: Optional[Callable[[optim.Optimizer], optim.lr_scheduler._LRScheduler]], wd_reg: Optional[WD_Regularization], learning_goal: float, min_lr: float, if_lr_adjust: bool, if_BN: bool, drop_rate: float) \\\n",
    "        -> List[Tuple[float, float, float, float]]:\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        model\n",
    "        opt\n",
    "        device\n",
    "        epochs\n",
    "        learing_rate\n",
    "        criterion\n",
    "        y: label of data\n",
    "        wd_reg, BN, DO: regularization\n",
    "    Results:\n",
    "        history: train_loss, train_accuracy, val_loss, val_accuracy of each epochs\n",
    "    \"\"\"\n",
    "    def forward_backward(optimizer: optim.Optimizer, criterion: nn.modules.loss._Loss, wd_reg: Optional[WD_Regularization], model: TwoLayerNetwork, y: torch.Tensor,\n",
    "                         BN: Optional[nn.modules.batchnorm._BatchNorm], DO: Optional[nn.modules.dropout._DropoutNd]) \\\n",
    "            -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            optimizer\n",
    "            criterion\n",
    "            model\n",
    "            y: label of data\n",
    "            wd_reg, BN, DO: regularization\n",
    "        Results:\n",
    "            ouputs: f(x)\n",
    "            loss_all: f(x) - y\n",
    "        \"\"\"\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        outputs = outputs if not DO else DO(outputs)\n",
    "        loss_all: torch.Tensor = criterion(\n",
    "            outputs, y) + wd_reg(model) if wd_reg else criterion(outputs, y)\n",
    "        loss_all.backward()\n",
    "        optimizer.step()\n",
    "        return loss_all, outputs\n",
    "    if epochs < 1:\n",
    "        raise ValueError(\"Invalid epoch!!\")\n",
    "    if not 0 <= drop_rate < 1:\n",
    "        raise ValueError(\"Invalid dropout rate!!\")\n",
    "    # init\n",
    "    epoch = 0\n",
    "    init_lr = learning_rate\n",
    "    origin_if_BN = model.if_BN\n",
    "    model.if_BN = if_BN\n",
    "    pre_loss = float(\"inf\") if if_lr_adjust else None\n",
    "    batch_norm = nn.BatchNorm1d(model.hidden_size).to(\n",
    "        device) if if_BN else None\n",
    "    drop_out = nn.Dropout(drop_rate).to(device) if drop_rate != 0. else None\n",
    "    model.to(device)\n",
    "    # if not model.storage[-1]\n",
    "    model.storage.append(list(model.parameters()))\n",
    "    optimizer = opt(model.storage[-1], lr=learning_rate)\n",
    "    scheduler = sched(optimizer) if sched else None\n",
    "    history = []\n",
    "    # Train the model\n",
    "    while epoch < epochs:\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        model.train()\n",
    "        for x, y in DataLoader(trainset, batch_size=32, shuffle=True):\n",
    "            x: torch.Tensor = x.view(-1, model.input_size).to(device)\n",
    "            y: torch.Tensor = y.to(device)\n",
    "            loss_all, outputs = forward_backward(\n",
    "                optimizer, criterion, wd_reg, model, y, batch_norm, drop_out)\n",
    "            # Learning rate adjustment\n",
    "            if pre_loss:\n",
    "                while pre_loss <= loss_all.item():\n",
    "                    # learning rate vanishing\n",
    "                    if learning_rate < min_lr:\n",
    "                        # return history\n",
    "                        learning_rate = init_lr\n",
    "                        optimizer = opt(model.storage[-1], lr=learning_rate)\n",
    "                        loss_all, outputs = forward_backward(\n",
    "                            optimizer, criterion, wd_reg, model, y, batch_norm, drop_out)\n",
    "                        # raise ValueError(f\"{learning_rate} < {min_lr}\")\n",
    "                        break\n",
    "                    learning_rate *= 0.7\n",
    "                    optimizer = opt(model.storage[-1], lr=learning_rate)\n",
    "                    loss_all, outputs = forward_backward(\n",
    "                        optimizer, criterion, wd_reg, model, y, batch_norm, drop_out)\n",
    "                learning_rate *= 1.2\n",
    "                pre_loss = loss_all.item()\n",
    "            train_loss += loss_all.item() * x.size(0)\n",
    "            predicted: torch.Tensor = torch.max(outputs.data, 1)[1]\n",
    "            train_correct += (predicted == y).sum().item()\n",
    "            model.storage.append(list(model.parameters()))\n",
    "        train_loss /= len(trainset)\n",
    "        train_accuracy = train_correct / len(trainset)\n",
    "        # Validate the model\n",
    "        val_loss, val_accuracy = validate(\n",
    "            model=model, device=device, valset=valset, criterion=criterion)\n",
    "        # Log statics\n",
    "        history.append((train_loss, train_accuracy, val_loss, val_accuracy))\n",
    "        # Stopping criteria\n",
    "        if learning_goal < val_accuracy:\n",
    "            return history\n",
    "        # Update loop\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        epoch += 1\n",
    "    # restore model\n",
    "    model.if_BN = origin_if_BN\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model: TwoLayerNetwork, device: str, testset: Dataset[torch.Tensor]) -> float:\n",
    "    \"\"\"return accuracy\"\"\"\n",
    "    return validate(model=model, device=device, valset=testset, criterion=nn.CrossEntropyLoss())[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogizing(model: TwoLayerNetwork, device: str, trainset: Dataset[torch.Tensor], learning_goal: float, criterion: nn.modules.loss._Loss):\n",
    "    x = torch.stack([x for x, _ in trainset]\n",
    "                    ).view(-1, model.input_size).to(\"cpu\")\n",
    "    y = torch.Tensor([y for _, y in trainset]).to(\"cpu\")\n",
    "    total_amount = len(x)\n",
    "    # get wrong correct indices\n",
    "    new_fc1_w = model.fc1.weight.data.to(device)\n",
    "    new_fc1_b = model.fc1.bias.data.to(device)\n",
    "    new_fc2_w = model.fc2.weight.data.to(device)\n",
    "    relu = nn.ReLU()\n",
    "    logits: torch.Tensor = x\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = relu(x.to(device) @ new_fc1_w.T + new_fc1_b\n",
    "                       ) @ new_fc2_w.T + model.fc2.bias.data.to(device)\n",
    "        logits, predicted = torch.max(outputs.data, 1)\n",
    "        success_condition = predicted == y.to(device)\n",
    "        wrong_indices = torch.nonzero(success_condition != True).to(\"cpu\")\n",
    "    #\n",
    "    init_func: Callable[[torch.Tensor], torch.Tensor] = lambda x: nn.init.xavier_uniform_(\n",
    "        tensor=x) if len(x.shape) > 1 else x\n",
    "    wrong_pointer = len(wrong_indices)\n",
    "    train_correct: int = 0\n",
    "    loss: float = float(\"inf\")\n",
    "    history: List[Tuple[float, float]] = []\n",
    "    while wrong_pointer > 0 and loss > learning_goal:\n",
    "        fc1 = nn.Linear(len(x[0]), 3).to(device)\n",
    "        fc2 = nn.Linear(3, product(model.fc2.bias.size()),\n",
    "                        False).to(device)\n",
    "        # with torch.no_grad():\n",
    "        target = torch.zeros(*torch.Size((total_amount,)))\n",
    "        wrong_pointer -= 1\n",
    "        pointer = wrong_indices[wrong_pointer]\n",
    "        catagory = int(y[pointer])\n",
    "        target[pointer] = catagory\n",
    "        target = target.to(device)\n",
    "        fc2.weight.data[:, :] = 0\n",
    "        fc2.weight.data[catagory, 0] = -2\n",
    "        fc2.weight.data[catagory, 1] = 1\n",
    "        fc2.weight.data[catagory, 2] = 1\n",
    "        delta = 0\n",
    "        intercept = 0\n",
    "        nonz = x\n",
    "        # randomly generate hyperplane which only contain the target x\n",
    "        while nonz.size() != (1, 2) or nonz.tolist()[0][0] != pointer:\n",
    "            for p in fc1.parameters():\n",
    "                init_func(p)\n",
    "            distances = x.to(device) @ fc1.weight.data[0].T\n",
    "            intercept = distances[pointer]\n",
    "            distances -= intercept\n",
    "            # get the shortest distance of other x to hyperplane\n",
    "            if (delta := torch.min(torch.abs(torch.cat(\n",
    "                    (distances[:pointer], distances[pointer + 1:])\n",
    "            )))) == 0:\n",
    "                continue\n",
    "            fc1.bias.data[1] = -intercept + (delta / 2)\n",
    "            fc1.bias.data[2] = -intercept - (delta / 2)\n",
    "            # check if delta too small for float32(default)\n",
    "            if fc1.bias.data[1] == fc1.bias.data[2]:\n",
    "                continue\n",
    "            fc1.bias.data[0] = -intercept\n",
    "            fc1.weight.data[1:] = fc1.weight.data[0]\n",
    "            outputs = relu(x.to(device) @ fc1.weight.data.T + fc1.bias.data\n",
    "                           ) @ fc2.weight.data.T\n",
    "            nonz = torch.nonzero(outputs)\n",
    "        # adjust weight in order to make the output of correct category greater than the others\n",
    "        fc2.weight.data *= logits[pointer].item(\n",
    "        ) / outputs[pointer].sum() + 1\n",
    "        new_fc1_w = torch.cat((new_fc1_w, fc1.weight.data)).to(device)\n",
    "        new_fc1_b = torch.cat((new_fc1_b, fc1.bias.data)).to(device)\n",
    "        new_fc2_w = torch.cat((new_fc2_w, fc2.weight.data), 1).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = relu(x.to(device) @ new_fc1_w.T + new_fc1_b\n",
    "                           ) @ new_fc2_w.T + model.fc2.bias.data.to(device)\n",
    "            loss = criterion(\n",
    "                outputs, y.to(device=device, dtype=torch.long)).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_correct = (predicted == y.to(device)).sum().item()\n",
    "        history.append((loss, train_correct / total_amount))\n",
    "    # construct new model\n",
    "    new_model = TwoLayerNetwork(model.input_size, len(new_fc1_b), product(\n",
    "        model.fc2.bias.size()), lambda _: _, lambda: model.active_func, model.do.p, model.if_BN)\n",
    "    for name, param in model.named_parameters():\n",
    "        layer_name, variable_type = name.split(\".\")\n",
    "        if layer_name == \"fc1\":\n",
    "            setattr(getattr(getattr(new_model, layer_name), variable_type),\n",
    "                    \"data\", eval(f\"new_{layer_name}_{variable_type[0]}\"))\n",
    "        elif layer_name == \"fc2\":\n",
    "            if variable_type == \"weight\":\n",
    "                setattr(getattr(getattr(new_model, layer_name), variable_type),\n",
    "                        \"data\", eval(f\"new_{layer_name}_{variable_type[0]}\"))\n",
    "            elif variable_type == \"bias\":\n",
    "                new_model.fc2.bias.data[:] = model.fc2.bias.data[:]\n",
    "            else:\n",
    "                pass\n",
    "                setattr(getattr(new_model, layer_name), variable_type, param)\n",
    "        else:\n",
    "            setattr(getattr(new_model, layer_name), variable_type, param)\n",
    "    return new_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pytorch dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "def getPytorchData(train: float = 0.8, remain: float = 0.1) \\\n",
    "        -> Tuple[Dataset[torch.Tensor], Dataset[torch.Tensor], Dataset[torch.Tensor], int, int]:\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        train: train_amount / total_amount or 1 - valid_amount / total_amount\n",
    "        remain: reduce data amount to save time\n",
    "    Results:\n",
    "        trainloader, valloader, testloader: dataloader\n",
    "        datum_size: size of datum\n",
    "        class_amount: amount of types\n",
    "    \"\"\"\n",
    "    # preprocess: flatten, normalize, drop 90%, split\n",
    "    transform = transforms.transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    if not 0 < train <= 1:\n",
    "        raise ValueError()\n",
    "    if not 0 < remain <= 1:\n",
    "        raise ValueError()\n",
    "    # Split the training set into training and validation sets\n",
    "    trainset = datasets.FashionMNIST(\n",
    "        root=\"./data/\", train=True, download=False, transform=transform)\n",
    "    train_count = int(train * remain * len(trainset))\n",
    "    valid_count = int((1 - train) * remain * len(trainset))\n",
    "    if train_count == 0 or valid_count == 0:\n",
    "        raise ValueError()\n",
    "    datum_size = product(trainset[0][0].size())\n",
    "    class_amount = len(trainset.classes)\n",
    "    testset = datasets.FashionMNIST(\n",
    "        root=\"./data/\", train=False, download=False, transform=transform)\n",
    "    print(train_count, valid_count, len(testset))\n",
    "    trainset, valset, _ = random_split(\n",
    "        trainset, (train_count, valid_count, len(trainset) - train_count - valid_count), Generator().manual_seed(42))\n",
    "    # Create dataloaders to load the data in batches\n",
    "    # trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "    # valloader = DataLoader(valset, batch_size=32, shuffle=True)\n",
    "    # testloader = DataLoader(testset, batch_size=32, shuffle=True)\n",
    "    return trainset, valset, testset, datum_size, class_amount\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WAN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800 1199 10000\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available(\n",
    ") else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "trainset, valset, testset, input_size, output_size = getPytorchData()\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1058\n",
      "[(2.122917129993439, 0.241875, 1.8850674040621773, 0.371976647206005), (1.8249681035677592, 0.37166666666666665, 1.6204896290367896, 0.4445371142618849), (1.560087955792745, 0.463125, 1.3555002062394284, 0.5754795663052544), (1.314503967364629, 0.6285416666666667, 1.1403866603137853, 0.6788990825688074), (1.1036804835001628, 0.6920833333333334, 0.9596145864920979, 0.7097581317764804), (0.9680635674794515, 0.7283333333333334, 0.8577197387976085, 0.7381150959132611), (0.8847484676043192, 0.7395833333333334, 0.7977340597724596, 0.7447873227689742), (0.8284859573841095, 0.75, 0.7567762555034087, 0.7481234361968306), (0.7906862459580104, 0.759375, 0.7200900295856498, 0.7698081734778982), (0.7592342120409011, 0.7695833333333333, 0.6948466327808815, 0.7764804003336113), (0.7355515336990357, 0.7725, 0.6783705689292634, 0.7689741451209341), (0.7138904406627019, 0.7764583333333334, 0.6551016444360543, 0.7748123436196831), (0.6969207811355591, 0.7833333333333333, 0.6392939514274693, 0.7856547122602169), (0.6792064893245697, 0.7854166666666667, 0.6251705039830084, 0.7889908256880734), (0.6665326376756032, 0.7933333333333333, 0.626277848369783, 0.7814845704753962), (0.6535676888624827, 0.795625, 0.6027088978570934, 0.7889908256880734), (0.6405239210526148, 0.7977083333333334, 0.6065899745885087, 0.7906588824020017), (0.6326478596528371, 0.8035416666666667, 0.5873074652355248, 0.7998331943286072), (0.6224743483463923, 0.8072916666666666, 0.5797953127919881, 0.8081734778982486)]\n",
      "0.7828\n"
     ]
    }
   ],
   "source": [
    "# hidden_size = 11\n",
    "# epochs = 300\n",
    "# init: Callable[[torch.Tensor], torch.Tensor] = lambda x: nn.init.xavier_uniform_(\n",
    "#     tensor=x) if len(x.shape) > 1 else x\n",
    "# active = nn.ReLU\n",
    "# model = TwoLayerNetwork(input_size, hidden_size,\n",
    "#                         output_size, init, active, 0., False)\n",
    "# optimize = optim.SGD\n",
    "# schedule = None\n",
    "# learning_goal = 0.8 #\n",
    "# learning_rate = 0.001\n",
    "# min_lr = learning_rate * 1e-5\n",
    "# l2_reg = L2_Regularization(0.001)\n",
    "# RG_EB_LG_UA_BN_DO_baseline = test(model, device, testset)\n",
    "# RG_EB_LG_UA_BN_DO_history = train(model, optimize, device, epochs, learning_rate,\n",
    "#                    trainset, valset, criterion, schedule, l2_reg, learning_goal, min_lr, True, False, 0.)\n",
    "# RG_EB_LG_UA_BN_DO_result = test(model, device, testset)\n",
    "# print(RG_EB_LG_UA_BN_DO_baseline, RG_EB_LG_UA_BN_DO_history, RG_EB_LG_UA_BN_DO_result, sep=\"\\n\")\n",
    "# model_path = r\"./data/rg_eb_lg_ua_\"\n",
    "# torch.save(model, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TwoLayerNetwork(\n",
       "  (do): Dropout(p=0.0, inplace=False)\n",
       "  (fc1): Linear(in_features=784, out_features=11, bias=True)\n",
       "  (bn1): BatchNorm1d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (active_func): ReLU()\n",
       "  (fc2): Linear(in_features=11, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = r\"./data/rg_eb_lg_ua_\"\n",
    "model: TwoLayerNetwork = torch.load(model_path)\n",
    "model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TwoLayerNetwork(\n",
       "  (do): Dropout(p=0.0, inplace=False)\n",
       "  (fc1): Linear(in_features=784, out_features=2063, bias=True)\n",
       "  (bn1): BatchNorm1d(2063, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (active_func): ReLU()\n",
       "  (fc2): Linear(in_features=2063, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = analogizing(model, device, trainset, 0.4, criterion)\n",
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_path = r\"./data/anlg_rg_eb_lg_ua_\"\n",
    "torch.save(new_model, new_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5762234632174174, 0.80625)\n",
      "(0.5797953142336352, 0.8081734778982486)\n",
      "(0.6225299583435059, 0.7828)\n",
      "(0.3981437009572983, 0.9222916666666666)\n",
      "(0.5982372802779314, 0.8006672226855713)\n",
      "(0.6369789076805115, 0.776)\n"
     ]
    }
   ],
   "source": [
    "print(validate(model, device, trainset, criterion))\n",
    "print(validate(model, device, valset, criterion))\n",
    "print(validate(model, device, testset, criterion))\n",
    "print(validate(new_model, device, trainset, criterion))\n",
    "print(validate(new_model, device, valset, criterion))\n",
    "print(validate(new_model, device, testset, criterion))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0725737be4be03859ccf648c604bdce2d511d4addba95219b9055f0ea318ae44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
